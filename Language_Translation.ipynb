{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\" Load Dataset from File \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(source_path, target_path, text_to_ids):\n",
    "    \"\"\" Preprocess Text Data.  Save to to file.\"\"\"\n",
    "    # Preprocess\n",
    "    source_text = load_data(source_path)\n",
    "    target_text = load_data(target_path)\n",
    "    \n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "    \n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "    \n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "# Save Data\n",
    "    with open('/Users/kamalesh_das/Desktop/Python/AcadGild/LanguageTranslator/preprocess.p', 'wb') as out_file:\n",
    "        pickle.dump((\n",
    "        (source_text, target_text),\n",
    "            (source_vocab_to_int, target_vocab_to_int),\n",
    "            (source_int_to_vocab, target_int_to_vocab)), out_file)\n",
    "\n",
    "\n",
    "def load_preprocess():\n",
    "    \"\"\"Load the Preprocessed Training data and return them in batches of <batch_size> or less\"\"\"\n",
    "    with open('/Users/kamalesh_das/Desktop/Python/AcadGild/LanguageTranslator/preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)\n",
    "\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"Create lookup tables for vocabulary\"\"\"\n",
    "    vocab = set(text.split())\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"Save parameters to file\"\"\"\n",
    "    with open('/Users/kamalesh_das/Desktop/Python/AcadGild/LanguageTranslator/params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"Load parameters from file\"\"\"\n",
    "    with open('/Users/kamalesh_das/Desktop/Python/AcadGild/LanguageTranslator/params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)\n",
    "\n",
    "\n",
    "def batch_data(source, target, batch_size):\n",
    "    \"\"\"Batch source and target together\"\"\"\n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        yield np.array(pad_sentence_batch(source_batch)), np.array(pad_sentence_batch(target_batch))\n",
    "\n",
    "\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentence with <PAD> id\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [CODES['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "\n",
    "def _print_success_message():\n",
    "    print('Tests Passed')\n",
    "\n",
    "\n",
    "def test_text_to_ids(text_to_ids):\n",
    "    test_source_text = 'new jersey is sometimes quiet during autumn , and it is snowy in april .\\nthe united states is usually chilly during july , and it is usually freezing in november .\\ncalifornia is usually quiet during march , and it is usually hot in june .\\nthe united states is sometimes mild during june , and it is cold in september .'\n",
    "    test_target_text = 'new jersey est parfois calme pendant l\\' automne , et il est neigeux en avril .\\nles états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\\ncalifornia est généralement calme en mars , et il est généralement chaud en juin .\\nles états-unis est parfois légère en juin , et il fait froid en septembre .'\n",
    "    \n",
    "    test_source_text = test_source_text.lower()\n",
    "    test_target_text = test_target_text.lower()\n",
    "    \n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(test_source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(test_target_text)\n",
    "    \n",
    "    test_source_id_seq, test_target_id_seq = text_to_ids(test_source_text, test_target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "    assert len(test_source_id_seq) == len(test_source_text.split('\\n')),\\\n",
    "        'source_id_text has wrong length, it should be {}.'.format(len(test_source_text.split('\\n')))\n",
    "    assert len(test_target_id_seq) == len(test_target_text.split('\\n')), \\\n",
    "        'target_id_text has wrong length, it should be {}.'.format(len(test_target_text.split('\\n')))\n",
    "\n",
    "    target_not_iter = [type(x) for x in test_source_id_seq if not isinstance(x, collections.Iterable)]\n",
    "    assert not target_not_iter,\\\n",
    "        'Element in source_id_text is not iteratable.  Found type {}'.format(target_not_iter[0])\n",
    "    target_not_iter = [type(x) for x in test_target_id_seq if not isinstance(x, collections.Iterable)]\n",
    "    assert not target_not_iter, \\\n",
    "        'Element in target_id_text is not iteratable.  Found type {}'.format(target_not_iter[0])\n",
    "\n",
    "    source_changed_length = [(words, word_ids) for words, word_ids in zip(test_source_text.split('\\n'), test_source_id_seq) if len(words.split()) != len(word_ids)]\n",
    "    assert not source_changed_length,\\\n",
    "        'Source text changed in size from {} word(s) to {} id(s): {}'.format(len(source_changed_length[0][0].split()), len(source_changed_length[0][1]), source_changed_length[0][1])\n",
    "\n",
    "    target_missing_end = [word_ids for word_ids in test_target_id_seq if word_ids[-1] != target_vocab_to_int['<EOS>']]\n",
    "    assert not target_missing_end,\\\n",
    "        'Missing <EOS> id at the end of {}'.format(target_missing_end[0])\n",
    "\n",
    "    target_bad_size = [(words.split(), word_ids) for words, word_ids in zip(test_target_text.split('\\n'), test_target_id_seq) if len(word_ids) != len(words.split()) + 1]\n",
    "    assert not target_bad_size,\\\n",
    "        'Target text incorrect size.  {} should be length {}'.format(target_bad_size[0][1], len(target_bad_size[0][0]) + 1)\n",
    "\n",
    "    source_bad_id = [(word, word_id) for word, word_id in zip([word for sentence in test_source_text.split('\\n') for word in sentence.split()],itertools.chain.from_iterable(test_source_id_seq)) if source_vocab_to_int[word] != word_id]\n",
    "    assert not source_bad_id,\\\n",
    "        'Source word incorrectly converted from {} to id {}.'.format(source_bad_id[0][0], source_bad_id[0][1])\n",
    "\n",
    "    target_bad_id = [(word, word_id) for word, word_id in zip([word for sentence in test_target_text.split('\\n') for word in sentence.split()],[word_id for word_ids in test_target_id_seq for word_id in word_ids[:-1]]) if target_vocab_to_int[word] != word_id]\n",
    "    assert not target_bad_id,\\\n",
    "        'Target word incorrectly converted from {} to id {}.'.format(target_bad_id[0][0], target_bad_id[0][1])\n",
    "\n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_model_inputs(model_inputs):\n",
    "    with tf.Graph().as_default():\n",
    "        input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()\n",
    "\n",
    "    # Check type\n",
    "        assert input_data.op.type == 'Placeholder',\\\n",
    "            'Input is not a Placeholder.'\n",
    "        assert targets.op.type == 'Placeholder',\\\n",
    "            'Targets is not a Placeholder.'\n",
    "        assert lr.op.type == 'Placeholder',\\\n",
    "            'Learning Rate is not a Placeholder.'\n",
    "        assert keep_prob.op.type == 'Placeholder', \\\n",
    "            'Keep Probability is not a Placeholder.'\n",
    "        assert target_sequence_length.op.type == 'Placeholder', \\\n",
    "            'Target Sequence Length is not a Placeholder.'\n",
    "        assert max_target_sequence_length.op.type == 'Max', \\\n",
    "            'Max Target Sequence Length is not a Placeholder.'\n",
    "        assert source_sequence_length.op.type == 'Placeholder', \\\n",
    "            'Source Sequence Length is not a Placeholder.'\n",
    "\n",
    "        # Check name\n",
    "        assert input_data.name == 'input:0',\\\n",
    "            'Input has bad name.  Found name {}'.format(input_data.name)\n",
    "        assert target_sequence_length.name == 'target_sequence_length:0',\\\n",
    "            'Target Sequence Length has bad name.  Found name {}'.format(target_sequence_length.name)\n",
    "        assert source_sequence_length.name == 'source_sequence_length:0',\\\n",
    "            'Source Sequence Length has bad name.  Found name {}'.format(source_sequence_length.name)\n",
    "        assert keep_prob.name == 'keep_prob:0', \\\n",
    "            'Keep Probability has bad name.  Found name {}'.format(keep_prob.name)\n",
    "\n",
    "        assert tf.assert_rank(input_data, 2, message='Input data has wrong rank')\n",
    "        assert tf.assert_rank(targets, 2, message='Targets has wrong rank')\n",
    "        assert tf.assert_rank(lr, 0, message='Learning Rate has wrong rank')\n",
    "        assert tf.assert_rank(keep_prob, 0, message='Keep Probability has wrong rank')\n",
    "        assert tf.assert_rank(target_sequence_length, 1, message='Target Sequence Length has wrong rank')\n",
    "        assert tf.assert_rank(max_target_sequence_length, 0, message='Max Target Sequence Length has wrong rank')\n",
    "        assert tf.assert_rank(source_sequence_length, 1, message='Source Sequence Lengthhas wrong rank')\n",
    "\n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_encoding_layer(encoding_layer):\n",
    "    rnn_size = 512\n",
    "    batch_size = 64\n",
    "    num_layers = 3\n",
    "    source_sequence_len = 22\n",
    "    source_vocab_size = 20\n",
    "    encoding_embedding_size = 30\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        rnn_inputs = tf.placeholder(tf.int32, [batch_size,source_sequence_len])\n",
    "        source_sequence_length = tf.placeholder(tf.int32,(None,),name='source_sequence_length')\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        enc_output, states = encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob,source_sequence_length, source_vocab_size,encoding_embedding_size)\n",
    "\n",
    "\n",
    "        assert len(states) == num_layers,\\\n",
    "            'Found {} state(s). It should be {} states.'.format(len(states), num_layers)\n",
    "\n",
    "        bad_types = [type(state) for state in states if not isinstance(state, tf.contrib.rnn.LSTMStateTuple)]\n",
    "        assert not bad_types,\\\n",
    "            'Found wrong type: {}'.format(bad_types[0])\n",
    "\n",
    "        bad_shapes = [state_tensor.get_shape() for state in states for state_tensor in state if state_tensor.get_shape().as_list() not in [[None, rnn_size], [batch_size, rnn_size]]]\n",
    "        assert not bad_shapes,\\\n",
    "            'Found wrong shape: {}'.format(bad_shapes[0])\n",
    "\n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_decoding_layer(decoding_layer):\n",
    "    batch_size = 64\n",
    "    vocab_size = 1000\n",
    "    embedding_size = 200\n",
    "    sequence_length = 22\n",
    "    rnn_size = 512\n",
    "    num_layers = 3\n",
    "    target_vocab_to_int = {'<EOS>': 1, '<GO>': 3}\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        target_sequence_length_p = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "        max_target_sequence_length = tf.reduce_max(target_sequence_length_p, name='max_target_len')\n",
    "\n",
    "        dec_input = tf.placeholder(tf.int32, [batch_size, sequence_length])\n",
    "        dec_embed_input = tf.placeholder(tf.float32, [batch_size, sequence_length, embedding_size])\n",
    "        dec_embeddings = tf.placeholder(tf.float32, [vocab_size, embedding_size])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        state = tf.contrib.rnn.LSTMStateTuple(tf.placeholder(tf.float32, [None, rnn_size]),tf.placeholder(tf.float32, [None, rnn_size]))\n",
    "        encoder_state = (state, state, state)\n",
    "\n",
    "        train_decoder_output, infer_logits_output = decoding_layer( dec_input,encoder_state,target_sequence_length_p,max_target_sequence_length,rnn_size,num_layers,target_vocab_to_int,vocab_size,batch_size,keep_prob,embedding_size)\n",
    "\n",
    "\n",
    "\n",
    "        assert isinstance(train_decoder_output, tf.contrib.seq2seq.BasicDecoderOutput),\\\n",
    "            'Found wrong type: {}'.format(type(train_decoder_output))\n",
    "        assert isinstance(infer_logits_output, tf.contrib.seq2seq.BasicDecoderOutput),\\\n",
    "            'Found wrong type: {}'.format(type(infer_logits_output))\n",
    "\n",
    "        assert train_decoder_output.rnn_output.get_shape().as_list() == [batch_size, None, vocab_size], \\\n",
    "            'Wrong shape returned.  Found {}'.format(train_decoder_output.rnn_output.get_shape())\n",
    "        assert infer_logits_output.sample_id.get_shape().as_list() == [batch_size, None], \\\n",
    "            'Wrong shape returned.  Found {}'.format(infer_logits_output.sample_id.get_shape())\n",
    "\n",
    "\n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_seq2seq_model(seq2seq_model):\n",
    "    batch_size = 64\n",
    "    vocab_size = 300\n",
    "    embedding_size = 100\n",
    "    sequence_length = 22\n",
    "    rnn_size = 512\n",
    "    num_layers = 3\n",
    "    target_vocab_to_int = {'<EOS>': 1, '<GO>': 3}\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        dec_input = tf.placeholder(tf.int32, [batch_size, sequence_length])\n",
    "        dec_embed_input = tf.placeholder(tf.float32, [batch_size, sequence_length, embedding_size])\n",
    "        dec_embeddings = tf.placeholder(tf.float32, [vocab_size, embedding_size])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        enc_state = tf.contrib.rnn.LSTMStateTuple(tf.placeholder(tf.float32, [None, rnn_size]),tf.placeholder(tf.float32, [None, rnn_size]))\n",
    "\n",
    "        input_data = tf.placeholder(tf.int32, [batch_size, sequence_length])\n",
    "        target_data = tf.placeholder(tf.int32, [batch_size, sequence_length])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "        target_sequence_length_p = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "        max_target_sequence_length = tf.reduce_max(target_sequence_length_p, name='max_target_len')\n",
    "\n",
    "        train_decoder_output, infer_logits_output = seq2seq_model(  input_data,target_data,keep_prob,batch_size,source_sequence_length,target_sequence_length_p,max_target_sequence_length,vocab_size,vocab_size,embedding_size,embedding_size,rnn_size,num_layers,target_vocab_to_int)\n",
    "\n",
    "        # input_data, target_data, keep_prob, batch_size, sequence_length,\n",
    "        # 200, target_vocab_size, 64, 80, rnn_size, num_layers, target_vocab_to_int)\n",
    "\n",
    "        assert isinstance(train_decoder_output, tf.contrib.seq2seq.BasicDecoderOutput),\\\n",
    "            'Found wrong type: {}'.format(type(train_decoder_output))\n",
    "        assert isinstance(infer_logits_output, tf.contrib.seq2seq.BasicDecoderOutput),\\\n",
    "            'Found wrong type: {}'.format(type(infer_logits_output))\n",
    "\n",
    "        assert train_decoder_output.rnn_output.get_shape().as_list() == [batch_size, None, vocab_size], \\\n",
    "            'Wrong shape returned.  Found {}'.format(train_decoder_output.rnn_output.get_shape())\n",
    "        assert infer_logits_output.sample_id.get_shape().as_list() == [batch_size, None], \\\n",
    "            'Wrong shape returned.  Found {}'.format(infer_logits_output.sample_id.get_shape())\n",
    "\n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_sentence_to_seq(sentence_to_seq):\n",
    "    sentence = 'this is a test sentence'\n",
    "    vocab_to_int = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, 'this': 3, 'is': 6, 'a': 5, 'sentence': 4}\n",
    "\n",
    "    output = sentence_to_seq(sentence, vocab_to_int)\n",
    "\n",
    "    assert len(output) == 5,\\\n",
    "        'Wrong length. Found a length of {}'.format(len(output))\n",
    "\n",
    "    assert output[3] == 2,\\\n",
    "        'Missing <UNK> id.'\n",
    "\n",
    "    assert np.array_equal(output, [3, 6, 5, 2, 4]),\\\n",
    "        'Incorrect ouput. Found {}'.format(output)\n",
    "\n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_process_encoding_input(process_encoding_input):\n",
    "    batch_size = 2\n",
    "    seq_length = 3\n",
    "    target_vocab_to_int = {'<GO>': 3}\n",
    "    with tf.Graph().as_default():\n",
    "        target_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "        dec_input = process_encoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "\n",
    "        assert dec_input.get_shape() == (batch_size, seq_length),\\\n",
    "            'Wrong shape returned.  Found {}'.format(dec_input.get_shape())\n",
    "\n",
    "        test_target_data = [[10, 20, 30], [40, 18, 23]]\n",
    "        with tf.Session() as sess:\n",
    "            test_dec_input = sess.run(dec_input, {target_data: test_target_data})\n",
    "\n",
    "        assert test_dec_input[0][0] == target_vocab_to_int['<GO>'] and\\\n",
    "            test_dec_input[1][0] == target_vocab_to_int['<GO>'],\\\n",
    "            'Missing GO Id.'\n",
    "\n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_decoding_layer_train(decoding_layer_train):\n",
    "    batch_size = 64\n",
    "    vocab_size = 1000\n",
    "    embedding_size = 200\n",
    "    sequence_length = 22\n",
    "    rnn_size = 512\n",
    "    num_layers = 3\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "# dec_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size)] * num_layers)\n",
    "\n",
    "            dec_embed_input = tf.placeholder(tf.float32, [batch_size, sequence_length, embedding_size])\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            target_sequence_length_p = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "            max_target_sequence_length = tf.reduce_max(target_sequence_length_p, name='max_target_len')\n",
    "\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "                    dec_cell = tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\n",
    "\n",
    "            output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1),name='output_layer')\n",
    "            # output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, None, scope=decoding_scope)\n",
    "\n",
    "\n",
    "            encoder_state = tf.contrib.rnn.LSTMStateTuple(tf.placeholder(tf.float32, [None, rnn_size]),tf.placeholder(tf.float32, [None, rnn_size]))\n",
    "\n",
    "            train_decoder_output = decoding_layer_train(encoder_state, dec_cell,dec_embed_input,target_sequence_length_p,max_target_sequence_length,output_layer,keep_prob)\n",
    "\n",
    "            # encoder_state, dec_cell, dec_embed_input, sequence_length,\n",
    "            #                      decoding_scope, output_fn, keep_prob)\n",
    "\n",
    "\n",
    "            assert isinstance(train_decoder_output, tf.contrib.seq2seq.BasicDecoderOutput),\\\n",
    "                'Found wrong type: {}'.format(type(train_decoder_output))\n",
    "\n",
    "            assert train_decoder_output.rnn_output.get_shape().as_list() == [batch_size, None, vocab_size], \\\n",
    "                'Wrong shape returned.  Found {}'.format(train_decoder_output.rnn_output.get_shape())\n",
    "\n",
    "    _print_success_message()\n",
    "\n",
    "\n",
    "def test_decoding_layer_infer(decoding_layer_infer):\n",
    "    batch_size = 64\n",
    "    vocab_size = 1000\n",
    "    sequence_length = 22\n",
    "    embedding_size = 200\n",
    "    rnn_size = 512\n",
    "    num_layers = 3\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "\n",
    "            dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size]))\n",
    "\n",
    "            dec_embed_input = tf.placeholder(tf.float32, [batch_size, sequence_length, embedding_size])\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            target_sequence_length_p = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "            max_target_sequence_length = tf.reduce_max(target_sequence_length_p, name='max_target_len')\n",
    "\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "                    dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "\n",
    "            output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1),name='output_layer')\n",
    "            # output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, None, scope=decoding_scope)\n",
    "\n",
    "\n",
    "            encoder_state = tf.contrib.rnn.LSTMStateTuple(tf.placeholder(tf.float32, [None, rnn_size]),tf.placeholder(tf.float32, [None, rnn_size]))\n",
    "\n",
    "            infer_logits_output = decoding_layer_infer( encoder_state,dec_cell,dec_embeddings,1,2,max_target_sequence_length,vocab_size,output_layer,batch_size,keep_prob)\n",
    "\n",
    "            # encoder_state, dec_cell, dec_embeddings, 10, 20,\n",
    "            #                     sequence_length, vocab_size, decoding_scope, output_fn, keep_prob)\n",
    "\n",
    "\n",
    "            assert isinstance(infer_logits_output, tf.contrib.seq2seq.BasicDecoderOutput),\\\n",
    "                'Found wrong type: {}'.format(type(infer_logits_output))\n",
    "\n",
    "            assert infer_logits_output.sample_id.get_shape().as_list() == [batch_size, None], \\\n",
    "                'Wrong shape returned.  Found {}'.format(infer_logits_output.sample_id.get_shape())\n",
    "\n",
    "    _print_success_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Main Code starts here #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = '/Users/kamalesh_das/Desktop/Python/AcadGild/LanguageTranslator/small_vocab_en'\n",
    "target_path = '/Users/kamalesh_das/Desktop/Python/AcadGild/LanguageTranslator/small_vocab_fr'\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 227\n",
      "Number of sentences: 40000\n",
      "Average number of words in a sentence: 13.2385\n",
      "\n",
      "English sentences 10 to 20:\n",
      "the lime is her least liked fruit , but the banana is my least liked .\n",
      "he saw a old yellow truck .\n",
      "india is rainy during june , and it is sometimes warm in november .\n",
      "that cat was my most loved animal .\n",
      "he dislikes grapefruit , limes , and lemons .\n",
      "her least liked fruit is the lemon , but his least liked is the grapefruit .\n",
      "california is never cold during february , but it is sometimes freezing in june .\n",
      "china is usually pleasant during autumn , and it is usually quiet in october .\n",
      "paris is never freezing during november , but it is wonderful in october .\n",
      "the united states is never rainy during january , but it is sometimes mild in october .\n",
      "\n",
      "French sentences 10 to 20:\n",
      "la chaux est son moins aimé des fruits , mais la banane est mon moins aimé.\n",
      "il a vu un vieux camion jaune .\n",
      "inde est pluvieux en juin , et il est parfois chaud en novembre .\n",
      "ce chat était mon animal le plus aimé .\n",
      "il n'aime pamplemousse , citrons verts et les citrons .\n",
      "son fruit est moins aimé le citron , mais son moins aimé est le pamplemousse .\n",
      "californie ne fait jamais froid en février , mais il est parfois le gel en juin .\n",
      "chine est généralement agréable en automne , et il est généralement calme en octobre .\n",
      "paris est jamais le gel en novembre , mais il est merveilleux en octobre .\n",
      "les états-unis est jamais pluvieux en janvier , mais il est parfois doux en octobre .\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (10, 20)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "##### Text to Word Ids\n",
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    # Just go through the text and transform it.\n",
    "    source_id_text = []\n",
    "    for idx, line in enumerate(source_text.split('\\n')):\n",
    "        source_id_text.append([])\n",
    "        for word in line.split():\n",
    "            source_id_text[idx].append(source_vocab_to_int[word])\n",
    "        \n",
    "    target_id_text = []\n",
    "    for idx, line in enumerate(target_text.split('\\n')):\n",
    "        target_id_text.append([])\n",
    "        for word in line.split():\n",
    "            target_id_text[idx].append(target_vocab_to_int[word])\n",
    "        target_id_text[idx].append(target_vocab_to_int['<EOS>'])\n",
    "\n",
    "    return (source_id_text, target_id_text)\n",
    "\n",
    "\n",
    "test_text_to_ids(text_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Preprocess all the data and save it\n",
    "preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Point\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "###Build the Neural Network\n",
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
    "    max target sequence length, source sequence length)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = tf.placeholder(tf.int32, shape=[None,None], name= \"input\")\n",
    "    targets = tf.placeholder(tf.int32, shape=[None,None], name= \"targets\")\n",
    "    lrate = tf.placeholder(tf.float32, name= \"learning_rate\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name= \"keep_prob\")\n",
    "    target_seq_lenth = tf.placeholder(tf.int32, shape=[None], name= \"target_sequence_length\")\n",
    "    max_target_len = tf.reduce_max(target_seq_lenth, name= 'max_target_len')\n",
    "    source_seq_length = tf.placeholder(tf.int32, shape=[None], name= \"source_sequence_length\")\n",
    "    return (inputs, targets, lrate, keep_prob, target_seq_lenth, max_target_len, source_seq_length)\n",
    "\n",
    "test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "### Process Decoder Input\n",
    "\n",
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # Create a constant tensor with the 'go id'.\n",
    "    go_id = tf.constant(target_vocab_to_int['<GO>'], shape=(batch_size,1), dtype=tf.int32)\n",
    "    # Concatenate the vector without the last word id with the go ids vector\n",
    "    processed_input = tf.concat([go_id,target_data[:,:-1]],1)\n",
    "    return processed_input\n",
    "\n",
    "test_process_encoding_input(process_decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "###Encoding\n",
    "\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param source_sequence_length: a list of the lengths of each sequence in the batch\n",
    "    :param source_vocab_size: vocabulary size of source data\n",
    "    :param encoding_embedding_size: embedding size of source data\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    # Build the lstm cells wrapped in dropout\n",
    "    def build_cell(rnn_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "        lstm_drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return lstm_drop\n",
    "    # Stack them all\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    # Creathe embedding layer.\n",
    "    embed_encoder = tf.contrib.layers.embed_sequence(rnn_inputs, vocab_size = source_vocab_size, embed_dim = encoding_embedding_size)\n",
    "    # If we don't have an initial zero state, provide a dtype.\n",
    "    output, state = tf.nn.dynamic_rnn(stacked_lstm, embed_encoder, source_sequence_length, dtype=tf.float32)\n",
    "    return (output, state)\n",
    "\n",
    "\n",
    "test_encoding_layer(encoding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "### Decoding - Training\n",
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
    "    :param max_summary_length: The length of the longest sequence in the batch\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    trainig_helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_sequence_length)\n",
    "    basic_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, trainig_helper, encoder_state, output_layer)\n",
    "    f_output, _, _ = tf.contrib.seq2seq.dynamic_decode(basic_decoder,maximum_iterations=max_summary_length)\n",
    "    return f_output\n",
    "\n",
    "\n",
    "test_decoding_layer_train(decoding_layer_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "### Decoding - Inference\n",
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param encoder_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_of_sequence_id: GO ID\n",
    "    :param end_of_sequence_id: EOS Id\n",
    "    :param max_target_sequence_length: Maximum length of target sequences\n",
    "    :param vocab_size: Size of decoder/target vocabulary\n",
    "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param batch_size: Batch size\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    # Convert the start_ids to be a vector with batch size (the go id repeated batch size times)\n",
    "    start_ids = tf.tile([start_of_sequence_id], [batch_size])\n",
    "    # Create the embedding helper.\n",
    "    embedding_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        dec_embeddings, start_ids, end_of_sequence_id)\n",
    "    basic_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        dec_cell, embedding_helper, encoder_state, output_layer)\n",
    "    f_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        basic_decoder,maximum_iterations=max_target_sequence_length)\n",
    "    return f_output\n",
    "\n",
    "\n",
    "test_decoding_layer_infer(decoding_layer_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "### Build the Decoding Layer\n",
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_input: Decoder input\n",
    "    :param encoder_state: Encoder state\n",
    "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
    "    :param max_target_sequence_length: Maximum length of target sequences\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param target_vocab_size: Size of target vocabulary\n",
    "    :param batch_size: The size of the batch\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param decoding_embedding_size: Decoding embedding size\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    # Use the same proess as in the encoding layer.\n",
    "    def build_cell(rnn_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "        lstm_drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return lstm_drop\n",
    "    # Stack them all\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    dense_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    with tf.variable_scope(\"decode\") as scope:\n",
    "        tr_decoder_output = decoding_layer_train(\n",
    "            encoder_state, stacked_lstm, dec_embed_input, \n",
    "            target_sequence_length, max_target_sequence_length, \n",
    "            dense_layer, keep_prob)\n",
    "        scope.reuse_variables()\n",
    "        inf_decoder_output = decoding_layer_infer(\n",
    "            encoder_state, stacked_lstm, dec_embeddings, \n",
    "            target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], \n",
    "            max_target_sequence_length, target_vocab_size, \n",
    "            dense_layer, batch_size, keep_prob)\n",
    "    \n",
    "    return tr_decoder_output, inf_decoder_output\n",
    "\n",
    "\n",
    "test_decoding_layer(decoding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "###### Build the Neural Network ############\n",
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param target_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param source_sequence_length: Sequence Lengths of source sequences in the batch\n",
    "    :param target_sequence_length: Sequence Lengths of target sequences in the batch\n",
    "    :param source_vocab_size: Source vocabulary size\n",
    "    :param target_vocab_size: Target vocabulary size\n",
    "    :param enc_embedding_size: Decoder embedding size\n",
    "    :param dec_embedding_size: Encoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    output, state = encoding_layer(input_data, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   enc_embedding_size)\n",
    "    \n",
    "    processed_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    tr_decoder_output, inf_decoder_output = decoding_layer(processed_input, state,\n",
    "                   target_sequence_length, max_target_sentence_length,\n",
    "                   rnn_size, num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, dec_embedding_size)\n",
    "    \n",
    "    return tr_decoder_output, inf_decoder_output\n",
    "\n",
    "\n",
    "\n",
    "test_seq2seq_model(seq2seq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network Training\n",
    "# Number of Epochs\n",
    "epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 512\n",
    "# RNN Size\n",
    "rnn_size = 128\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 128\n",
    "decoding_embedding_size = 128\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.55\n",
    "display_step = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build the Graph\n",
    "save_path = '/Users/kamalesh_das/Desktop/Python/AcadGild/LanguageTranslator/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()\n",
    "\n",
    "    #sequence_length = tf.placeholder_with_default(max_target_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    1/78 - Train Accuracy: 0.2329, Validation Accuracy: 0.3096, Loss: 5.6255\n",
      "Epoch   0 Batch    2/78 - Train Accuracy: 0.2655, Validation Accuracy: 0.3096, Loss: 5.3936\n",
      "Epoch   0 Batch    3/78 - Train Accuracy: 0.2444, Validation Accuracy: 0.3096, Loss: 5.2271\n",
      "Epoch   0 Batch    4/78 - Train Accuracy: 0.2317, Validation Accuracy: 0.3096, Loss: 5.0593\n",
      "Epoch   0 Batch    5/78 - Train Accuracy: 0.2380, Validation Accuracy: 0.3121, Loss: 4.8916\n",
      "Epoch   0 Batch    6/78 - Train Accuracy: 0.2843, Validation Accuracy: 0.3155, Loss: 4.5755\n",
      "Epoch   0 Batch    7/78 - Train Accuracy: 0.2837, Validation Accuracy: 0.3155, Loss: 4.4486\n",
      "Epoch   0 Batch    8/78 - Train Accuracy: 0.2494, Validation Accuracy: 0.3169, Loss: 4.5276\n",
      "Epoch   0 Batch    9/78 - Train Accuracy: 0.2876, Validation Accuracy: 0.3279, Loss: 4.3126\n",
      "Epoch   0 Batch   10/78 - Train Accuracy: 0.2740, Validation Accuracy: 0.3448, Loss: 4.3580\n",
      "Epoch   0 Batch   11/78 - Train Accuracy: 0.3121, Validation Accuracy: 0.3469, Loss: 4.0852\n",
      "Epoch   0 Batch   12/78 - Train Accuracy: 0.2874, Validation Accuracy: 0.3479, Loss: 4.1214\n",
      "Epoch   0 Batch   13/78 - Train Accuracy: 0.3499, Validation Accuracy: 0.3461, Loss: 3.7803\n",
      "Epoch   0 Batch   14/78 - Train Accuracy: 0.3053, Validation Accuracy: 0.3428, Loss: 3.8860\n",
      "Epoch   0 Batch   15/78 - Train Accuracy: 0.2960, Validation Accuracy: 0.3428, Loss: 3.8322\n",
      "Epoch   0 Batch   16/78 - Train Accuracy: 0.3118, Validation Accuracy: 0.3428, Loss: 3.6902\n",
      "Epoch   0 Batch   17/78 - Train Accuracy: 0.3048, Validation Accuracy: 0.3454, Loss: 3.6589\n",
      "Epoch   0 Batch   18/78 - Train Accuracy: 0.3007, Validation Accuracy: 0.3683, Loss: 3.7417\n",
      "Epoch   0 Batch   19/78 - Train Accuracy: 0.3785, Validation Accuracy: 0.3794, Loss: 3.3742\n",
      "Epoch   0 Batch   20/78 - Train Accuracy: 0.3181, Validation Accuracy: 0.3823, Loss: 3.5902\n",
      "Epoch   0 Batch   21/78 - Train Accuracy: 0.3186, Validation Accuracy: 0.3823, Loss: 3.5472\n",
      "Epoch   0 Batch   22/78 - Train Accuracy: 0.3554, Validation Accuracy: 0.3825, Loss: 3.3391\n",
      "Epoch   0 Batch   23/78 - Train Accuracy: 0.3651, Validation Accuracy: 0.3833, Loss: 3.2715\n",
      "Epoch   0 Batch   24/78 - Train Accuracy: 0.3209, Validation Accuracy: 0.3884, Loss: 3.4236\n",
      "Epoch   0 Batch   25/78 - Train Accuracy: 0.3263, Validation Accuracy: 0.3887, Loss: 3.3891\n",
      "Epoch   0 Batch   26/78 - Train Accuracy: 0.3896, Validation Accuracy: 0.3896, Loss: 3.0647\n",
      "Epoch   0 Batch   27/78 - Train Accuracy: 0.3554, Validation Accuracy: 0.3896, Loss: 3.1876\n",
      "Epoch   0 Batch   28/78 - Train Accuracy: 0.3166, Validation Accuracy: 0.3923, Loss: 3.3577\n",
      "Epoch   0 Batch   29/78 - Train Accuracy: 0.3294, Validation Accuracy: 0.3968, Loss: 3.2841\n",
      "Epoch   0 Batch   30/78 - Train Accuracy: 0.3623, Validation Accuracy: 0.3997, Loss: 3.1247\n",
      "Epoch   0 Batch   31/78 - Train Accuracy: 0.3817, Validation Accuracy: 0.4064, Loss: 3.0691\n",
      "Epoch   0 Batch   32/78 - Train Accuracy: 0.3766, Validation Accuracy: 0.4154, Loss: 3.0773\n",
      "Epoch   0 Batch   33/78 - Train Accuracy: 0.3770, Validation Accuracy: 0.4051, Loss: 3.0009\n",
      "Epoch   0 Batch   34/78 - Train Accuracy: 0.3780, Validation Accuracy: 0.4088, Loss: 3.0304\n",
      "Epoch   0 Batch   35/78 - Train Accuracy: 0.3855, Validation Accuracy: 0.4126, Loss: 2.9860\n",
      "Epoch   0 Batch   36/78 - Train Accuracy: 0.3868, Validation Accuracy: 0.4162, Loss: 2.9902\n",
      "Epoch   0 Batch   37/78 - Train Accuracy: 0.3831, Validation Accuracy: 0.4094, Loss: 2.9657\n",
      "Epoch   0 Batch   38/78 - Train Accuracy: 0.3851, Validation Accuracy: 0.4168, Loss: 2.9718\n",
      "Epoch   0 Batch   39/78 - Train Accuracy: 0.3887, Validation Accuracy: 0.4209, Loss: 2.9350\n",
      "Epoch   0 Batch   40/78 - Train Accuracy: 0.3619, Validation Accuracy: 0.4203, Loss: 3.0420\n",
      "Epoch   0 Batch   41/78 - Train Accuracy: 0.3904, Validation Accuracy: 0.4232, Loss: 2.9113\n",
      "Epoch   0 Batch   42/78 - Train Accuracy: 0.4186, Validation Accuracy: 0.4201, Loss: 2.7715\n",
      "Epoch   0 Batch   43/78 - Train Accuracy: 0.3792, Validation Accuracy: 0.4257, Loss: 2.9887\n",
      "Epoch   0 Batch   44/78 - Train Accuracy: 0.4090, Validation Accuracy: 0.4283, Loss: 2.8509\n",
      "Epoch   0 Batch   45/78 - Train Accuracy: 0.3757, Validation Accuracy: 0.4259, Loss: 2.9734\n",
      "Epoch   0 Batch   46/78 - Train Accuracy: 0.3741, Validation Accuracy: 0.4318, Loss: 3.0015\n",
      "Epoch   0 Batch   47/78 - Train Accuracy: 0.4392, Validation Accuracy: 0.4438, Loss: 2.6855\n",
      "Epoch   0 Batch   48/78 - Train Accuracy: 0.4107, Validation Accuracy: 0.4314, Loss: 2.7965\n",
      "Epoch   0 Batch   49/78 - Train Accuracy: 0.3876, Validation Accuracy: 0.4411, Loss: 2.9154\n",
      "Epoch   0 Batch   50/78 - Train Accuracy: 0.4002, Validation Accuracy: 0.4520, Loss: 2.9033\n",
      "Epoch   0 Batch   51/78 - Train Accuracy: 0.4169, Validation Accuracy: 0.4456, Loss: 2.8024\n",
      "Epoch   0 Batch   52/78 - Train Accuracy: 0.4317, Validation Accuracy: 0.4498, Loss: 2.7530\n",
      "Epoch   0 Batch   53/78 - Train Accuracy: 0.3950, Validation Accuracy: 0.4492, Loss: 2.8751\n",
      "Epoch   0 Batch   54/78 - Train Accuracy: 0.4036, Validation Accuracy: 0.4475, Loss: 2.8653\n",
      "Epoch   0 Batch   55/78 - Train Accuracy: 0.4275, Validation Accuracy: 0.4548, Loss: 2.7334\n",
      "Epoch   0 Batch   56/78 - Train Accuracy: 0.4354, Validation Accuracy: 0.4568, Loss: 2.7158\n",
      "Epoch   0 Batch   57/78 - Train Accuracy: 0.4336, Validation Accuracy: 0.4577, Loss: 2.7042\n",
      "Epoch   0 Batch   58/78 - Train Accuracy: 0.4390, Validation Accuracy: 0.4623, Loss: 2.6936\n",
      "Epoch   0 Batch   59/78 - Train Accuracy: 0.4311, Validation Accuracy: 0.4563, Loss: 2.6843\n",
      "Epoch   0 Batch   60/78 - Train Accuracy: 0.4457, Validation Accuracy: 0.4634, Loss: 2.6072\n",
      "Epoch   0 Batch   61/78 - Train Accuracy: 0.4582, Validation Accuracy: 0.4536, Loss: 2.5354\n",
      "Epoch   0 Batch   62/78 - Train Accuracy: 0.4635, Validation Accuracy: 0.4616, Loss: 2.5505\n",
      "Epoch   0 Batch   63/78 - Train Accuracy: 0.4385, Validation Accuracy: 0.4624, Loss: 2.6225\n",
      "Epoch   0 Batch   64/78 - Train Accuracy: 0.4393, Validation Accuracy: 0.4636, Loss: 2.6380\n",
      "Epoch   0 Batch   65/78 - Train Accuracy: 0.4435, Validation Accuracy: 0.4675, Loss: 2.6127\n",
      "Epoch   0 Batch   66/78 - Train Accuracy: 0.4432, Validation Accuracy: 0.4461, Loss: 2.5377\n",
      "Epoch   0 Batch   67/78 - Train Accuracy: 0.4333, Validation Accuracy: 0.4678, Loss: 2.6296\n",
      "Epoch   0 Batch   68/78 - Train Accuracy: 0.3836, Validation Accuracy: 0.4019, Loss: 2.6619\n",
      "Epoch   0 Batch   69/78 - Train Accuracy: 0.4106, Validation Accuracy: 0.4703, Loss: 2.8326\n",
      "Epoch   0 Batch   70/78 - Train Accuracy: 0.4466, Validation Accuracy: 0.4664, Loss: 2.5635\n",
      "Epoch   0 Batch   71/78 - Train Accuracy: 0.4037, Validation Accuracy: 0.4574, Loss: 2.7560\n",
      "Epoch   0 Batch   72/78 - Train Accuracy: 0.4268, Validation Accuracy: 0.4273, Loss: 2.4796\n",
      "Epoch   0 Batch   73/78 - Train Accuracy: 0.4556, Validation Accuracy: 0.4782, Loss: 2.6035\n",
      "Epoch   0 Batch   74/78 - Train Accuracy: 0.4218, Validation Accuracy: 0.4726, Loss: 2.6387\n",
      "Epoch   0 Batch   75/78 - Train Accuracy: 0.4448, Validation Accuracy: 0.4696, Loss: 2.5631\n",
      "Epoch   0 Batch   76/78 - Train Accuracy: 0.4109, Validation Accuracy: 0.4404, Loss: 2.5784\n",
      "Epoch   1 Batch    1/78 - Train Accuracy: 0.4146, Validation Accuracy: 0.4767, Loss: 2.6425\n",
      "Epoch   1 Batch    2/78 - Train Accuracy: 0.4501, Validation Accuracy: 0.4832, Loss: 2.5743\n",
      "Epoch   1 Batch    3/78 - Train Accuracy: 0.4229, Validation Accuracy: 0.4656, Loss: 2.6186\n",
      "Epoch   1 Batch    4/78 - Train Accuracy: 0.4299, Validation Accuracy: 0.4826, Loss: 2.6174\n",
      "Epoch   1 Batch    5/78 - Train Accuracy: 0.4247, Validation Accuracy: 0.4806, Loss: 2.6267\n",
      "Epoch   1 Batch    6/78 - Train Accuracy: 0.4623, Validation Accuracy: 0.4768, Loss: 2.4301\n",
      "Epoch   1 Batch    7/78 - Train Accuracy: 0.4488, Validation Accuracy: 0.4702, Loss: 2.4514\n",
      "Epoch   1 Batch    8/78 - Train Accuracy: 0.4361, Validation Accuracy: 0.4817, Loss: 2.5739\n",
      "Epoch   1 Batch    9/78 - Train Accuracy: 0.4508, Validation Accuracy: 0.4832, Loss: 2.4791\n",
      "Epoch   1 Batch   10/78 - Train Accuracy: 0.4343, Validation Accuracy: 0.4815, Loss: 2.5520\n",
      "Epoch   1 Batch   11/78 - Train Accuracy: 0.4517, Validation Accuracy: 0.4794, Loss: 2.4626\n",
      "Epoch   1 Batch   12/78 - Train Accuracy: 0.4241, Validation Accuracy: 0.4730, Loss: 2.5366\n",
      "Epoch   1 Batch   13/78 - Train Accuracy: 0.4789, Validation Accuracy: 0.4728, Loss: 2.2763\n",
      "Epoch   1 Batch   14/78 - Train Accuracy: 0.4528, Validation Accuracy: 0.4759, Loss: 2.4324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch   15/78 - Train Accuracy: 0.4467, Validation Accuracy: 0.4806, Loss: 2.4604\n",
      "Epoch   1 Batch   16/78 - Train Accuracy: 0.4622, Validation Accuracy: 0.4801, Loss: 2.4087\n",
      "Epoch   1 Batch   17/78 - Train Accuracy: 0.4593, Validation Accuracy: 0.4885, Loss: 2.3914\n",
      "Epoch   1 Batch   18/78 - Train Accuracy: 0.4351, Validation Accuracy: 0.4853, Loss: 2.4933\n",
      "Epoch   1 Batch   19/78 - Train Accuracy: 0.4869, Validation Accuracy: 0.4776, Loss: 2.2889\n",
      "Epoch   1 Batch   20/78 - Train Accuracy: 0.4378, Validation Accuracy: 0.4763, Loss: 2.4682\n",
      "Epoch   1 Batch   21/78 - Train Accuracy: 0.4397, Validation Accuracy: 0.4849, Loss: 2.4947\n",
      "Epoch   1 Batch   22/78 - Train Accuracy: 0.4647, Validation Accuracy: 0.4864, Loss: 2.3601\n",
      "Epoch   1 Batch   23/78 - Train Accuracy: 0.4702, Validation Accuracy: 0.4877, Loss: 2.3414\n",
      "Epoch   1 Batch   24/78 - Train Accuracy: 0.4380, Validation Accuracy: 0.4859, Loss: 2.4602\n",
      "Epoch   1 Batch   25/78 - Train Accuracy: 0.4323, Validation Accuracy: 0.4806, Loss: 2.4518\n",
      "Epoch   1 Batch   26/78 - Train Accuracy: 0.4922, Validation Accuracy: 0.4805, Loss: 2.2042\n",
      "Epoch   1 Batch   27/78 - Train Accuracy: 0.4595, Validation Accuracy: 0.4862, Loss: 2.3506\n",
      "Epoch   1 Batch   28/78 - Train Accuracy: 0.4263, Validation Accuracy: 0.4917, Loss: 2.4966\n",
      "Epoch   1 Batch   29/78 - Train Accuracy: 0.4425, Validation Accuracy: 0.4896, Loss: 2.4360\n",
      "Epoch   1 Batch   30/78 - Train Accuracy: 0.4697, Validation Accuracy: 0.4964, Loss: 2.3196\n",
      "Epoch   1 Batch   31/78 - Train Accuracy: 0.4645, Validation Accuracy: 0.4831, Loss: 2.2913\n",
      "Epoch   1 Batch   32/78 - Train Accuracy: 0.4535, Validation Accuracy: 0.4818, Loss: 2.3082\n",
      "Epoch   1 Batch   33/78 - Train Accuracy: 0.4668, Validation Accuracy: 0.4834, Loss: 2.2575\n",
      "Epoch   1 Batch   34/78 - Train Accuracy: 0.4636, Validation Accuracy: 0.4859, Loss: 2.2830\n",
      "Epoch   1 Batch   35/78 - Train Accuracy: 0.4706, Validation Accuracy: 0.4885, Loss: 2.2643\n",
      "Epoch   1 Batch   36/78 - Train Accuracy: 0.4656, Validation Accuracy: 0.4871, Loss: 2.2835\n",
      "Epoch   1 Batch   37/78 - Train Accuracy: 0.4633, Validation Accuracy: 0.4853, Loss: 2.2785\n",
      "Epoch   1 Batch   38/78 - Train Accuracy: 0.4628, Validation Accuracy: 0.4869, Loss: 2.2726\n",
      "Epoch   1 Batch   39/78 - Train Accuracy: 0.4685, Validation Accuracy: 0.4932, Loss: 2.2589\n",
      "Epoch   1 Batch   40/78 - Train Accuracy: 0.4451, Validation Accuracy: 0.4918, Loss: 2.3305\n",
      "Epoch   1 Batch   41/78 - Train Accuracy: 0.4590, Validation Accuracy: 0.4897, Loss: 2.2524\n",
      "Epoch   1 Batch   42/78 - Train Accuracy: 0.4914, Validation Accuracy: 0.4936, Loss: 2.1621\n",
      "Epoch   1 Batch   43/78 - Train Accuracy: 0.4485, Validation Accuracy: 0.4944, Loss: 2.2939\n",
      "Epoch   1 Batch   44/78 - Train Accuracy: 0.4744, Validation Accuracy: 0.4917, Loss: 2.2012\n",
      "Epoch   1 Batch   45/78 - Train Accuracy: 0.4534, Validation Accuracy: 0.4962, Loss: 2.3068\n",
      "Epoch   1 Batch   46/78 - Train Accuracy: 0.4455, Validation Accuracy: 0.4960, Loss: 2.3323\n",
      "Epoch   1 Batch   47/78 - Train Accuracy: 0.5055, Validation Accuracy: 0.4975, Loss: 2.0849\n",
      "Epoch   1 Batch   48/78 - Train Accuracy: 0.4780, Validation Accuracy: 0.4951, Loss: 2.1464\n",
      "Epoch   1 Batch   49/78 - Train Accuracy: 0.4468, Validation Accuracy: 0.4921, Loss: 2.2649\n",
      "Epoch   1 Batch   50/78 - Train Accuracy: 0.4451, Validation Accuracy: 0.4922, Loss: 2.2802\n",
      "Epoch   1 Batch   51/78 - Train Accuracy: 0.4596, Validation Accuracy: 0.4902, Loss: 2.2095\n",
      "Epoch   1 Batch   52/78 - Train Accuracy: 0.4608, Validation Accuracy: 0.4893, Loss: 2.1465\n",
      "Epoch   1 Batch   53/78 - Train Accuracy: 0.4500, Validation Accuracy: 0.4909, Loss: 2.2511\n",
      "Epoch   1 Batch   54/78 - Train Accuracy: 0.4466, Validation Accuracy: 0.4927, Loss: 2.2688\n",
      "Epoch   1 Batch   55/78 - Train Accuracy: 0.4706, Validation Accuracy: 0.4914, Loss: 2.1518\n",
      "Epoch   1 Batch   56/78 - Train Accuracy: 0.4776, Validation Accuracy: 0.4944, Loss: 2.1318\n",
      "Epoch   1 Batch   57/78 - Train Accuracy: 0.4692, Validation Accuracy: 0.4902, Loss: 2.1349\n",
      "Epoch   1 Batch   58/78 - Train Accuracy: 0.4726, Validation Accuracy: 0.4949, Loss: 2.1311\n",
      "Epoch   1 Batch   59/78 - Train Accuracy: 0.4782, Validation Accuracy: 0.4928, Loss: 2.0930\n",
      "Epoch   1 Batch   60/78 - Train Accuracy: 0.4781, Validation Accuracy: 0.4985, Loss: 2.0536\n",
      "Epoch   1 Batch   61/78 - Train Accuracy: 0.5022, Validation Accuracy: 0.5007, Loss: 1.9896\n",
      "Epoch   1 Batch   62/78 - Train Accuracy: 0.5003, Validation Accuracy: 0.5028, Loss: 2.0268\n",
      "Epoch   1 Batch   63/78 - Train Accuracy: 0.4715, Validation Accuracy: 0.4972, Loss: 2.0853\n",
      "Epoch   1 Batch   64/78 - Train Accuracy: 0.4586, Validation Accuracy: 0.4967, Loss: 2.0917\n",
      "Epoch   1 Batch   65/78 - Train Accuracy: 0.4739, Validation Accuracy: 0.4938, Loss: 2.0651\n",
      "Epoch   1 Batch   66/78 - Train Accuracy: 0.4739, Validation Accuracy: 0.4932, Loss: 2.0144\n",
      "Epoch   1 Batch   67/78 - Train Accuracy: 0.4613, Validation Accuracy: 0.4967, Loss: 2.0788\n",
      "Epoch   1 Batch   68/78 - Train Accuracy: 0.4612, Validation Accuracy: 0.4972, Loss: 2.0639\n",
      "Epoch   1 Batch   69/78 - Train Accuracy: 0.4352, Validation Accuracy: 0.4892, Loss: 2.1978\n",
      "Epoch   1 Batch   70/78 - Train Accuracy: 0.4825, Validation Accuracy: 0.4973, Loss: 2.0400\n",
      "Epoch   1 Batch   71/78 - Train Accuracy: 0.4477, Validation Accuracy: 0.4968, Loss: 2.1459\n",
      "Epoch   1 Batch   72/78 - Train Accuracy: 0.4814, Validation Accuracy: 0.4911, Loss: 1.9617\n",
      "Epoch   1 Batch   73/78 - Train Accuracy: 0.4694, Validation Accuracy: 0.4991, Loss: 2.0438\n",
      "Epoch   1 Batch   74/78 - Train Accuracy: 0.4492, Validation Accuracy: 0.4949, Loss: 2.0664\n",
      "Epoch   1 Batch   75/78 - Train Accuracy: 0.4646, Validation Accuracy: 0.4941, Loss: 1.9930\n",
      "Epoch   1 Batch   76/78 - Train Accuracy: 0.4624, Validation Accuracy: 0.4983, Loss: 2.0329\n",
      "Epoch   2 Batch    1/78 - Train Accuracy: 0.4415, Validation Accuracy: 0.4996, Loss: 2.0815\n",
      "Epoch   2 Batch    2/78 - Train Accuracy: 0.4616, Validation Accuracy: 0.4998, Loss: 2.0066\n",
      "Epoch   2 Batch    3/78 - Train Accuracy: 0.4512, Validation Accuracy: 0.5018, Loss: 2.0544\n",
      "Epoch   2 Batch    4/78 - Train Accuracy: 0.4581, Validation Accuracy: 0.5058, Loss: 2.0463\n",
      "Epoch   2 Batch    5/78 - Train Accuracy: 0.4365, Validation Accuracy: 0.4957, Loss: 2.0680\n",
      "Epoch   2 Batch    6/78 - Train Accuracy: 0.4768, Validation Accuracy: 0.4966, Loss: 1.8824\n",
      "Epoch   2 Batch    7/78 - Train Accuracy: 0.4881, Validation Accuracy: 0.5065, Loss: 1.9192\n",
      "Epoch   2 Batch    8/78 - Train Accuracy: 0.4583, Validation Accuracy: 0.5073, Loss: 2.0029\n",
      "Epoch   2 Batch    9/78 - Train Accuracy: 0.4812, Validation Accuracy: 0.5077, Loss: 1.9343\n",
      "Epoch   2 Batch   10/78 - Train Accuracy: 0.4709, Validation Accuracy: 0.5107, Loss: 1.9712\n",
      "Epoch   2 Batch   11/78 - Train Accuracy: 0.4854, Validation Accuracy: 0.5085, Loss: 1.9088\n",
      "Epoch   2 Batch   12/78 - Train Accuracy: 0.4685, Validation Accuracy: 0.5051, Loss: 1.9755\n",
      "Epoch   2 Batch   13/78 - Train Accuracy: 0.5159, Validation Accuracy: 0.5035, Loss: 1.7678\n",
      "Epoch   2 Batch   14/78 - Train Accuracy: 0.4797, Validation Accuracy: 0.5067, Loss: 1.8751\n",
      "Epoch   2 Batch   15/78 - Train Accuracy: 0.4724, Validation Accuracy: 0.5081, Loss: 1.8800\n",
      "Epoch   2 Batch   16/78 - Train Accuracy: 0.4912, Validation Accuracy: 0.5079, Loss: 1.8644\n",
      "Epoch   2 Batch   17/78 - Train Accuracy: 0.4788, Validation Accuracy: 0.5064, Loss: 1.8300\n",
      "Epoch   2 Batch   18/78 - Train Accuracy: 0.4597, Validation Accuracy: 0.5074, Loss: 1.9153\n",
      "Epoch   2 Batch   19/78 - Train Accuracy: 0.5172, Validation Accuracy: 0.5142, Loss: 1.7408\n",
      "Epoch   2 Batch   20/78 - Train Accuracy: 0.4775, Validation Accuracy: 0.5173, Loss: 1.8935\n",
      "Epoch   2 Batch   21/78 - Train Accuracy: 0.4718, Validation Accuracy: 0.5178, Loss: 1.9253\n",
      "Epoch   2 Batch   22/78 - Train Accuracy: 0.4980, Validation Accuracy: 0.5173, Loss: 1.7962\n",
      "Epoch   2 Batch   23/78 - Train Accuracy: 0.5063, Validation Accuracy: 0.5194, Loss: 1.8100\n",
      "Epoch   2 Batch   24/78 - Train Accuracy: 0.4669, Validation Accuracy: 0.5134, Loss: 1.8714\n",
      "Epoch   2 Batch   25/78 - Train Accuracy: 0.4575, Validation Accuracy: 0.5085, Loss: 1.8876\n",
      "Epoch   2 Batch   26/78 - Train Accuracy: 0.5170, Validation Accuracy: 0.5139, Loss: 1.6938\n",
      "Epoch   2 Batch   27/78 - Train Accuracy: 0.4918, Validation Accuracy: 0.5178, Loss: 1.7939\n",
      "Epoch   2 Batch   28/78 - Train Accuracy: 0.4509, Validation Accuracy: 0.5191, Loss: 1.8918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch   29/78 - Train Accuracy: 0.4521, Validation Accuracy: 0.5036, Loss: 1.8451\n",
      "Epoch   2 Batch   30/78 - Train Accuracy: 0.4871, Validation Accuracy: 0.5075, Loss: 1.7799\n",
      "Epoch   2 Batch   31/78 - Train Accuracy: 0.4923, Validation Accuracy: 0.5162, Loss: 1.7672\n",
      "Epoch   2 Batch   32/78 - Train Accuracy: 0.4833, Validation Accuracy: 0.5159, Loss: 1.7615\n",
      "Epoch   2 Batch   33/78 - Train Accuracy: 0.4959, Validation Accuracy: 0.5120, Loss: 1.6945\n",
      "Epoch   2 Batch   34/78 - Train Accuracy: 0.4907, Validation Accuracy: 0.5138, Loss: 1.7319\n",
      "Epoch   2 Batch   35/78 - Train Accuracy: 0.5042, Validation Accuracy: 0.5210, Loss: 1.7103\n",
      "Epoch   2 Batch   36/78 - Train Accuracy: 0.4985, Validation Accuracy: 0.5247, Loss: 1.7210\n",
      "Epoch   2 Batch   37/78 - Train Accuracy: 0.5046, Validation Accuracy: 0.5255, Loss: 1.7292\n",
      "Epoch   2 Batch   38/78 - Train Accuracy: 0.5007, Validation Accuracy: 0.5312, Loss: 1.7168\n",
      "Epoch   2 Batch   39/78 - Train Accuracy: 0.5073, Validation Accuracy: 0.5292, Loss: 1.6931\n",
      "Epoch   2 Batch   40/78 - Train Accuracy: 0.4753, Validation Accuracy: 0.5233, Loss: 1.7559\n",
      "Epoch   2 Batch   41/78 - Train Accuracy: 0.5005, Validation Accuracy: 0.5249, Loss: 1.6992\n",
      "Epoch   2 Batch   42/78 - Train Accuracy: 0.5316, Validation Accuracy: 0.5309, Loss: 1.6131\n",
      "Epoch   2 Batch   43/78 - Train Accuracy: 0.4784, Validation Accuracy: 0.5217, Loss: 1.7335\n",
      "Epoch   2 Batch   44/78 - Train Accuracy: 0.5113, Validation Accuracy: 0.5293, Loss: 1.6667\n",
      "Epoch   2 Batch   45/78 - Train Accuracy: 0.4858, Validation Accuracy: 0.5328, Loss: 1.7342\n",
      "Epoch   2 Batch   46/78 - Train Accuracy: 0.4767, Validation Accuracy: 0.5255, Loss: 1.7304\n",
      "Epoch   2 Batch   47/78 - Train Accuracy: 0.5335, Validation Accuracy: 0.5320, Loss: 1.5614\n",
      "Epoch   2 Batch   48/78 - Train Accuracy: 0.5083, Validation Accuracy: 0.5294, Loss: 1.6134\n",
      "Epoch   2 Batch   49/78 - Train Accuracy: 0.4726, Validation Accuracy: 0.5202, Loss: 1.6998\n",
      "Epoch   2 Batch   50/78 - Train Accuracy: 0.4911, Validation Accuracy: 0.5327, Loss: 1.7182\n",
      "Epoch   2 Batch   51/78 - Train Accuracy: 0.5045, Validation Accuracy: 0.5349, Loss: 1.6683\n",
      "Epoch   2 Batch   52/78 - Train Accuracy: 0.5010, Validation Accuracy: 0.5265, Loss: 1.6033\n",
      "Epoch   2 Batch   53/78 - Train Accuracy: 0.4865, Validation Accuracy: 0.5325, Loss: 1.6997\n",
      "Epoch   2 Batch   54/78 - Train Accuracy: 0.4970, Validation Accuracy: 0.5358, Loss: 1.6950\n",
      "Epoch   2 Batch   55/78 - Train Accuracy: 0.5170, Validation Accuracy: 0.5387, Loss: 1.6045\n",
      "Epoch   2 Batch   56/78 - Train Accuracy: 0.5243, Validation Accuracy: 0.5386, Loss: 1.6075\n",
      "Epoch   2 Batch   57/78 - Train Accuracy: 0.5201, Validation Accuracy: 0.5376, Loss: 1.6134\n",
      "Epoch   2 Batch   58/78 - Train Accuracy: 0.5184, Validation Accuracy: 0.5387, Loss: 1.5896\n",
      "Epoch   2 Batch   59/78 - Train Accuracy: 0.5180, Validation Accuracy: 0.5364, Loss: 1.5752\n",
      "Epoch   2 Batch   60/78 - Train Accuracy: 0.5196, Validation Accuracy: 0.5341, Loss: 1.5335\n",
      "Epoch   2 Batch   61/78 - Train Accuracy: 0.5396, Validation Accuracy: 0.5360, Loss: 1.5018\n",
      "Epoch   2 Batch   62/78 - Train Accuracy: 0.5314, Validation Accuracy: 0.5368, Loss: 1.5341\n",
      "Epoch   2 Batch   63/78 - Train Accuracy: 0.5102, Validation Accuracy: 0.5382, Loss: 1.5674\n",
      "Epoch   2 Batch   64/78 - Train Accuracy: 0.5047, Validation Accuracy: 0.5371, Loss: 1.5672\n",
      "Epoch   2 Batch   65/78 - Train Accuracy: 0.5149, Validation Accuracy: 0.5329, Loss: 1.5553\n",
      "Epoch   2 Batch   66/78 - Train Accuracy: 0.5150, Validation Accuracy: 0.5298, Loss: 1.5167\n",
      "Epoch   2 Batch   67/78 - Train Accuracy: 0.4977, Validation Accuracy: 0.5281, Loss: 1.5799\n",
      "Epoch   2 Batch   68/78 - Train Accuracy: 0.4951, Validation Accuracy: 0.5259, Loss: 1.5625\n",
      "Epoch   2 Batch   69/78 - Train Accuracy: 0.4753, Validation Accuracy: 0.5259, Loss: 1.6770\n",
      "Epoch   2 Batch   70/78 - Train Accuracy: 0.5145, Validation Accuracy: 0.5277, Loss: 1.5488\n",
      "Epoch   2 Batch   71/78 - Train Accuracy: 0.4826, Validation Accuracy: 0.5272, Loss: 1.6188\n",
      "Epoch   2 Batch   72/78 - Train Accuracy: 0.5147, Validation Accuracy: 0.5230, Loss: 1.4879\n",
      "Epoch   2 Batch   73/78 - Train Accuracy: 0.5002, Validation Accuracy: 0.5233, Loss: 1.5561\n",
      "Epoch   2 Batch   74/78 - Train Accuracy: 0.4853, Validation Accuracy: 0.5249, Loss: 1.5719\n",
      "Epoch   2 Batch   75/78 - Train Accuracy: 0.4931, Validation Accuracy: 0.5258, Loss: 1.5183\n",
      "Epoch   2 Batch   76/78 - Train Accuracy: 0.4867, Validation Accuracy: 0.5232, Loss: 1.5435\n",
      "Epoch   3 Batch    1/78 - Train Accuracy: 0.4824, Validation Accuracy: 0.5276, Loss: 1.5645\n",
      "Epoch   3 Batch    2/78 - Train Accuracy: 0.4870, Validation Accuracy: 0.5271, Loss: 1.5316\n",
      "Epoch   3 Batch    3/78 - Train Accuracy: 0.4812, Validation Accuracy: 0.5273, Loss: 1.5668\n",
      "Epoch   3 Batch    4/78 - Train Accuracy: 0.4704, Validation Accuracy: 0.5234, Loss: 1.5588\n",
      "Epoch   3 Batch    5/78 - Train Accuracy: 0.4637, Validation Accuracy: 0.5214, Loss: 1.5769\n",
      "Epoch   3 Batch    6/78 - Train Accuracy: 0.5024, Validation Accuracy: 0.5218, Loss: 1.4338\n",
      "Epoch   3 Batch    7/78 - Train Accuracy: 0.5020, Validation Accuracy: 0.5204, Loss: 1.4790\n",
      "Epoch   3 Batch    8/78 - Train Accuracy: 0.4698, Validation Accuracy: 0.5177, Loss: 1.5386\n",
      "Epoch   3 Batch    9/78 - Train Accuracy: 0.4780, Validation Accuracy: 0.5184, Loss: 1.4835\n",
      "Epoch   3 Batch   10/78 - Train Accuracy: 0.4601, Validation Accuracy: 0.5098, Loss: 1.5106\n",
      "Epoch   3 Batch   11/78 - Train Accuracy: 0.4832, Validation Accuracy: 0.5099, Loss: 1.4794\n",
      "Epoch   3 Batch   12/78 - Train Accuracy: 0.4641, Validation Accuracy: 0.5123, Loss: 1.5379\n",
      "Epoch   3 Batch   13/78 - Train Accuracy: 0.5241, Validation Accuracy: 0.5146, Loss: 1.3729\n",
      "Epoch   3 Batch   14/78 - Train Accuracy: 0.4885, Validation Accuracy: 0.5177, Loss: 1.4561\n",
      "Epoch   3 Batch   15/78 - Train Accuracy: 0.4821, Validation Accuracy: 0.5210, Loss: 1.4539\n",
      "Epoch   3 Batch   16/78 - Train Accuracy: 0.5142, Validation Accuracy: 0.5199, Loss: 1.4580\n",
      "Epoch   3 Batch   17/78 - Train Accuracy: 0.4979, Validation Accuracy: 0.5224, Loss: 1.4270\n",
      "Epoch   3 Batch   18/78 - Train Accuracy: 0.4749, Validation Accuracy: 0.5175, Loss: 1.4915\n",
      "Epoch   3 Batch   19/78 - Train Accuracy: 0.5274, Validation Accuracy: 0.5200, Loss: 1.3671\n",
      "Epoch   3 Batch   20/78 - Train Accuracy: 0.4825, Validation Accuracy: 0.5204, Loss: 1.4870\n",
      "Epoch   3 Batch   21/78 - Train Accuracy: 0.4758, Validation Accuracy: 0.5191, Loss: 1.5191\n",
      "Epoch   3 Batch   22/78 - Train Accuracy: 0.5030, Validation Accuracy: 0.5205, Loss: 1.4192\n",
      "Epoch   3 Batch   23/78 - Train Accuracy: 0.5056, Validation Accuracy: 0.5187, Loss: 1.4316\n",
      "Epoch   3 Batch   24/78 - Train Accuracy: 0.4763, Validation Accuracy: 0.5184, Loss: 1.4730\n",
      "Epoch   3 Batch   25/78 - Train Accuracy: 0.4731, Validation Accuracy: 0.5198, Loss: 1.4758\n",
      "Epoch   3 Batch   26/78 - Train Accuracy: 0.5239, Validation Accuracy: 0.5179, Loss: 1.3287\n",
      "Epoch   3 Batch   27/78 - Train Accuracy: 0.4984, Validation Accuracy: 0.5207, Loss: 1.4182\n",
      "Epoch   3 Batch   28/78 - Train Accuracy: 0.4568, Validation Accuracy: 0.5199, Loss: 1.4926\n",
      "Epoch   3 Batch   29/78 - Train Accuracy: 0.4745, Validation Accuracy: 0.5203, Loss: 1.4497\n",
      "Epoch   3 Batch   30/78 - Train Accuracy: 0.5022, Validation Accuracy: 0.5202, Loss: 1.4024\n",
      "Epoch   3 Batch   31/78 - Train Accuracy: 0.5067, Validation Accuracy: 0.5188, Loss: 1.3897\n",
      "Epoch   3 Batch   32/78 - Train Accuracy: 0.4935, Validation Accuracy: 0.5200, Loss: 1.4070\n",
      "Epoch   3 Batch   33/78 - Train Accuracy: 0.5113, Validation Accuracy: 0.5200, Loss: 1.3671\n",
      "Epoch   3 Batch   34/78 - Train Accuracy: 0.5063, Validation Accuracy: 0.5225, Loss: 1.3767\n",
      "Epoch   3 Batch   35/78 - Train Accuracy: 0.5191, Validation Accuracy: 0.5297, Loss: 1.3750\n",
      "Epoch   3 Batch   36/78 - Train Accuracy: 0.5100, Validation Accuracy: 0.5303, Loss: 1.3852\n",
      "Epoch   3 Batch   37/78 - Train Accuracy: 0.5088, Validation Accuracy: 0.5245, Loss: 1.3886\n",
      "Epoch   3 Batch   38/78 - Train Accuracy: 0.4974, Validation Accuracy: 0.5257, Loss: 1.3842\n",
      "Epoch   3 Batch   39/78 - Train Accuracy: 0.5073, Validation Accuracy: 0.5221, Loss: 1.3642\n",
      "Epoch   3 Batch   40/78 - Train Accuracy: 0.4664, Validation Accuracy: 0.5137, Loss: 1.4132\n",
      "Epoch   3 Batch   41/78 - Train Accuracy: 0.5085, Validation Accuracy: 0.5266, Loss: 1.3752\n",
      "Epoch   3 Batch   42/78 - Train Accuracy: 0.5289, Validation Accuracy: 0.5237, Loss: 1.2937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3 Batch   43/78 - Train Accuracy: 0.4683, Validation Accuracy: 0.5130, Loss: 1.4049\n",
      "Epoch   3 Batch   44/78 - Train Accuracy: 0.5065, Validation Accuracy: 0.5226, Loss: 1.3491\n",
      "Epoch   3 Batch   45/78 - Train Accuracy: 0.4885, Validation Accuracy: 0.5248, Loss: 1.4053\n",
      "Epoch   3 Batch   46/78 - Train Accuracy: 0.4805, Validation Accuracy: 0.5195, Loss: 1.4036\n",
      "Epoch   3 Batch   47/78 - Train Accuracy: 0.5257, Validation Accuracy: 0.5203, Loss: 1.2656\n",
      "Epoch   3 Batch   48/78 - Train Accuracy: 0.5076, Validation Accuracy: 0.5306, Loss: 1.3056\n",
      "Epoch   3 Batch   49/78 - Train Accuracy: 0.4751, Validation Accuracy: 0.5134, Loss: 1.3814\n",
      "Epoch   3 Batch   50/78 - Train Accuracy: 0.4636, Validation Accuracy: 0.5098, Loss: 1.3868\n",
      "Epoch   3 Batch   51/78 - Train Accuracy: 0.4905, Validation Accuracy: 0.5196, Loss: 1.3670\n",
      "Epoch   3 Batch   52/78 - Train Accuracy: 0.5038, Validation Accuracy: 0.5241, Loss: 1.3084\n",
      "Epoch   3 Batch   53/78 - Train Accuracy: 0.4747, Validation Accuracy: 0.5178, Loss: 1.3909\n",
      "Epoch   3 Batch   54/78 - Train Accuracy: 0.4909, Validation Accuracy: 0.5217, Loss: 1.3874\n",
      "Epoch   3 Batch   55/78 - Train Accuracy: 0.4975, Validation Accuracy: 0.5144, Loss: 1.2998\n",
      "Epoch   3 Batch   56/78 - Train Accuracy: 0.5076, Validation Accuracy: 0.5162, Loss: 1.3199\n",
      "Epoch   3 Batch   57/78 - Train Accuracy: 0.5036, Validation Accuracy: 0.5195, Loss: 1.3312\n",
      "Epoch   3 Batch   58/78 - Train Accuracy: 0.4992, Validation Accuracy: 0.5172, Loss: 1.3024\n",
      "Epoch   3 Batch   59/78 - Train Accuracy: 0.4961, Validation Accuracy: 0.5166, Loss: 1.2853\n",
      "Epoch   3 Batch   60/78 - Train Accuracy: 0.5062, Validation Accuracy: 0.5168, Loss: 1.2625\n",
      "Epoch   3 Batch   61/78 - Train Accuracy: 0.5166, Validation Accuracy: 0.5150, Loss: 1.2361\n",
      "Epoch   3 Batch   62/78 - Train Accuracy: 0.5193, Validation Accuracy: 0.5162, Loss: 1.2575\n",
      "Epoch   3 Batch   63/78 - Train Accuracy: 0.4886, Validation Accuracy: 0.5210, Loss: 1.2991\n",
      "Epoch   3 Batch   64/78 - Train Accuracy: 0.5037, Validation Accuracy: 0.5198, Loss: 1.2811\n",
      "Epoch   3 Batch   65/78 - Train Accuracy: 0.4992, Validation Accuracy: 0.5101, Loss: 1.2647\n",
      "Epoch   3 Batch   66/78 - Train Accuracy: 0.5040, Validation Accuracy: 0.5159, Loss: 1.2383\n",
      "Epoch   3 Batch   67/78 - Train Accuracy: 0.4928, Validation Accuracy: 0.5168, Loss: 1.2957\n",
      "Epoch   3 Batch   68/78 - Train Accuracy: 0.4760, Validation Accuracy: 0.5118, Loss: 1.2945\n",
      "Epoch   3 Batch   69/78 - Train Accuracy: 0.4665, Validation Accuracy: 0.5126, Loss: 1.3727\n",
      "Epoch   3 Batch   70/78 - Train Accuracy: 0.5018, Validation Accuracy: 0.5103, Loss: 1.2712\n",
      "Epoch   3 Batch   71/78 - Train Accuracy: 0.4782, Validation Accuracy: 0.5163, Loss: 1.3340\n",
      "Epoch   3 Batch   72/78 - Train Accuracy: 0.5160, Validation Accuracy: 0.5244, Loss: 1.2262\n",
      "Epoch   3 Batch   73/78 - Train Accuracy: 0.5102, Validation Accuracy: 0.5289, Loss: 1.2922\n",
      "Epoch   3 Batch   74/78 - Train Accuracy: 0.4896, Validation Accuracy: 0.5281, Loss: 1.2863\n",
      "Epoch   3 Batch   75/78 - Train Accuracy: 0.5073, Validation Accuracy: 0.5358, Loss: 1.2604\n",
      "Epoch   3 Batch   76/78 - Train Accuracy: 0.4958, Validation Accuracy: 0.5342, Loss: 1.2785\n",
      "Epoch   4 Batch    1/78 - Train Accuracy: 0.4771, Validation Accuracy: 0.5302, Loss: 1.2929\n",
      "Epoch   4 Batch    2/78 - Train Accuracy: 0.4992, Validation Accuracy: 0.5327, Loss: 1.2636\n",
      "Epoch   4 Batch    3/78 - Train Accuracy: 0.4863, Validation Accuracy: 0.5310, Loss: 1.2882\n",
      "Epoch   4 Batch    4/78 - Train Accuracy: 0.4833, Validation Accuracy: 0.5320, Loss: 1.2806\n",
      "Epoch   4 Batch    5/78 - Train Accuracy: 0.4748, Validation Accuracy: 0.5305, Loss: 1.2939\n",
      "Epoch   4 Batch    6/78 - Train Accuracy: 0.5223, Validation Accuracy: 0.5368, Loss: 1.1891\n",
      "Epoch   4 Batch    7/78 - Train Accuracy: 0.5093, Validation Accuracy: 0.5321, Loss: 1.2193\n",
      "Epoch   4 Batch    8/78 - Train Accuracy: 0.4843, Validation Accuracy: 0.5321, Loss: 1.2789\n",
      "Epoch   4 Batch    9/78 - Train Accuracy: 0.5004, Validation Accuracy: 0.5327, Loss: 1.2324\n",
      "Epoch   4 Batch   10/78 - Train Accuracy: 0.4938, Validation Accuracy: 0.5376, Loss: 1.2451\n",
      "Epoch   4 Batch   11/78 - Train Accuracy: 0.5080, Validation Accuracy: 0.5393, Loss: 1.2252\n",
      "Epoch   4 Batch   12/78 - Train Accuracy: 0.4840, Validation Accuracy: 0.5339, Loss: 1.2650\n",
      "Epoch   4 Batch   13/78 - Train Accuracy: 0.5327, Validation Accuracy: 0.5360, Loss: 1.1339\n",
      "Epoch   4 Batch   14/78 - Train Accuracy: 0.5028, Validation Accuracy: 0.5340, Loss: 1.2005\n",
      "Epoch   4 Batch   15/78 - Train Accuracy: 0.5063, Validation Accuracy: 0.5385, Loss: 1.2015\n",
      "Epoch   4 Batch   16/78 - Train Accuracy: 0.5225, Validation Accuracy: 0.5320, Loss: 1.2068\n",
      "Epoch   4 Batch   17/78 - Train Accuracy: 0.5182, Validation Accuracy: 0.5303, Loss: 1.1664\n",
      "Epoch   4 Batch   18/78 - Train Accuracy: 0.4879, Validation Accuracy: 0.5232, Loss: 1.2336\n",
      "Epoch   4 Batch   19/78 - Train Accuracy: 0.5329, Validation Accuracy: 0.5291, Loss: 1.1257\n",
      "Epoch   4 Batch   20/78 - Train Accuracy: 0.4968, Validation Accuracy: 0.5275, Loss: 1.2267\n",
      "Epoch   4 Batch   21/78 - Train Accuracy: 0.4856, Validation Accuracy: 0.5273, Loss: 1.2587\n",
      "Epoch   4 Batch   22/78 - Train Accuracy: 0.5071, Validation Accuracy: 0.5304, Loss: 1.1666\n",
      "Epoch   4 Batch   23/78 - Train Accuracy: 0.5234, Validation Accuracy: 0.5353, Loss: 1.1930\n",
      "Epoch   4 Batch   24/78 - Train Accuracy: 0.4941, Validation Accuracy: 0.5343, Loss: 1.2225\n",
      "Epoch   4 Batch   25/78 - Train Accuracy: 0.4831, Validation Accuracy: 0.5298, Loss: 1.2337\n",
      "Epoch   4 Batch   26/78 - Train Accuracy: 0.5366, Validation Accuracy: 0.5231, Loss: 1.0977\n",
      "Epoch   4 Batch   27/78 - Train Accuracy: 0.5068, Validation Accuracy: 0.5249, Loss: 1.1673\n",
      "Epoch   4 Batch   28/78 - Train Accuracy: 0.4841, Validation Accuracy: 0.5297, Loss: 1.2310\n",
      "Epoch   4 Batch   29/78 - Train Accuracy: 0.4867, Validation Accuracy: 0.5278, Loss: 1.2019\n",
      "Epoch   4 Batch   30/78 - Train Accuracy: 0.5093, Validation Accuracy: 0.5221, Loss: 1.1546\n",
      "Epoch   4 Batch   31/78 - Train Accuracy: 0.5226, Validation Accuracy: 0.5315, Loss: 1.1485\n",
      "Epoch   4 Batch   32/78 - Train Accuracy: 0.4976, Validation Accuracy: 0.5262, Loss: 1.1521\n",
      "Epoch   4 Batch   33/78 - Train Accuracy: 0.5218, Validation Accuracy: 0.5233, Loss: 1.1219\n",
      "Epoch   4 Batch   34/78 - Train Accuracy: 0.5164, Validation Accuracy: 0.5276, Loss: 1.1447\n",
      "Epoch   4 Batch   35/78 - Train Accuracy: 0.5175, Validation Accuracy: 0.5268, Loss: 1.1455\n",
      "Epoch   4 Batch   36/78 - Train Accuracy: 0.5054, Validation Accuracy: 0.5169, Loss: 1.1500\n",
      "Epoch   4 Batch   37/78 - Train Accuracy: 0.5019, Validation Accuracy: 0.5247, Loss: 1.1562\n",
      "Epoch   4 Batch   38/78 - Train Accuracy: 0.5140, Validation Accuracy: 0.5289, Loss: 1.1462\n",
      "Epoch   4 Batch   39/78 - Train Accuracy: 0.5314, Validation Accuracy: 0.5352, Loss: 1.1251\n",
      "Epoch   4 Batch   40/78 - Train Accuracy: 0.4977, Validation Accuracy: 0.5336, Loss: 1.1714\n",
      "Epoch   4 Batch   41/78 - Train Accuracy: 0.5244, Validation Accuracy: 0.5314, Loss: 1.1349\n",
      "Epoch   4 Batch   42/78 - Train Accuracy: 0.5340, Validation Accuracy: 0.5325, Loss: 1.0743\n",
      "Epoch   4 Batch   43/78 - Train Accuracy: 0.5231, Validation Accuracy: 0.5500, Loss: 1.1601\n",
      "Epoch   4 Batch   44/78 - Train Accuracy: 0.5439, Validation Accuracy: 0.5426, Loss: 1.1216\n",
      "Epoch   4 Batch   45/78 - Train Accuracy: 0.5021, Validation Accuracy: 0.5423, Loss: 1.1695\n",
      "Epoch   4 Batch   46/78 - Train Accuracy: 0.5207, Validation Accuracy: 0.5469, Loss: 1.1639\n",
      "Epoch   4 Batch   47/78 - Train Accuracy: 0.5644, Validation Accuracy: 0.5431, Loss: 1.0472\n",
      "Epoch   4 Batch   48/78 - Train Accuracy: 0.5458, Validation Accuracy: 0.5466, Loss: 1.0871\n",
      "Epoch   4 Batch   49/78 - Train Accuracy: 0.5150, Validation Accuracy: 0.5492, Loss: 1.1458\n",
      "Epoch   4 Batch   50/78 - Train Accuracy: 0.5120, Validation Accuracy: 0.5439, Loss: 1.1586\n",
      "Epoch   4 Batch   51/78 - Train Accuracy: 0.5237, Validation Accuracy: 0.5413, Loss: 1.1241\n",
      "Epoch   4 Batch   52/78 - Train Accuracy: 0.5250, Validation Accuracy: 0.5420, Loss: 1.0761\n",
      "Epoch   4 Batch   53/78 - Train Accuracy: 0.5250, Validation Accuracy: 0.5480, Loss: 1.1566\n",
      "Epoch   4 Batch   54/78 - Train Accuracy: 0.5287, Validation Accuracy: 0.5563, Loss: 1.1439\n",
      "Epoch   4 Batch   55/78 - Train Accuracy: 0.5408, Validation Accuracy: 0.5544, Loss: 1.0931\n",
      "Epoch   4 Batch   56/78 - Train Accuracy: 0.5300, Validation Accuracy: 0.5361, Loss: 1.0951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 Batch   57/78 - Train Accuracy: 0.5203, Validation Accuracy: 0.5303, Loss: 1.0998\n",
      "Epoch   4 Batch   58/78 - Train Accuracy: 0.5288, Validation Accuracy: 0.5452, Loss: 1.0888\n",
      "Epoch   4 Batch   59/78 - Train Accuracy: 0.5319, Validation Accuracy: 0.5439, Loss: 1.0644\n",
      "Epoch   4 Batch   60/78 - Train Accuracy: 0.5214, Validation Accuracy: 0.5305, Loss: 1.0480\n",
      "Epoch   4 Batch   61/78 - Train Accuracy: 0.5399, Validation Accuracy: 0.5367, Loss: 1.0197\n",
      "Epoch   4 Batch   62/78 - Train Accuracy: 0.5387, Validation Accuracy: 0.5405, Loss: 1.0535\n",
      "Epoch   4 Batch   63/78 - Train Accuracy: 0.5067, Validation Accuracy: 0.5411, Loss: 1.0828\n",
      "Epoch   4 Batch   64/78 - Train Accuracy: 0.5314, Validation Accuracy: 0.5526, Loss: 1.0707\n",
      "Epoch   4 Batch   65/78 - Train Accuracy: 0.5370, Validation Accuracy: 0.5511, Loss: 1.0587\n",
      "Epoch   4 Batch   66/78 - Train Accuracy: 0.5388, Validation Accuracy: 0.5474, Loss: 1.0313\n",
      "Epoch   4 Batch   67/78 - Train Accuracy: 0.5154, Validation Accuracy: 0.5395, Loss: 1.0834\n",
      "Epoch   4 Batch   68/78 - Train Accuracy: 0.5141, Validation Accuracy: 0.5372, Loss: 1.0781\n",
      "Epoch   4 Batch   69/78 - Train Accuracy: 0.5021, Validation Accuracy: 0.5427, Loss: 1.1558\n",
      "Epoch   4 Batch   70/78 - Train Accuracy: 0.5469, Validation Accuracy: 0.5510, Loss: 1.0559\n",
      "Epoch   4 Batch   71/78 - Train Accuracy: 0.5042, Validation Accuracy: 0.5444, Loss: 1.1084\n",
      "Epoch   4 Batch   72/78 - Train Accuracy: 0.5415, Validation Accuracy: 0.5462, Loss: 1.0324\n",
      "Epoch   4 Batch   73/78 - Train Accuracy: 0.5350, Validation Accuracy: 0.5525, Loss: 1.0836\n",
      "Epoch   4 Batch   74/78 - Train Accuracy: 0.5358, Validation Accuracy: 0.5565, Loss: 1.0779\n",
      "Epoch   4 Batch   75/78 - Train Accuracy: 0.5339, Validation Accuracy: 0.5486, Loss: 1.0491\n",
      "Epoch   4 Batch   76/78 - Train Accuracy: 0.5183, Validation Accuracy: 0.5473, Loss: 1.0693\n",
      "Epoch   5 Batch    1/78 - Train Accuracy: 0.5188, Validation Accuracy: 0.5533, Loss: 1.0838\n",
      "Epoch   5 Batch    2/78 - Train Accuracy: 0.5147, Validation Accuracy: 0.5482, Loss: 1.0592\n",
      "Epoch   5 Batch    3/78 - Train Accuracy: 0.5315, Validation Accuracy: 0.5518, Loss: 1.0812\n",
      "Epoch   5 Batch    4/78 - Train Accuracy: 0.5195, Validation Accuracy: 0.5496, Loss: 1.0714\n",
      "Epoch   5 Batch    5/78 - Train Accuracy: 0.5041, Validation Accuracy: 0.5492, Loss: 1.0817\n",
      "Epoch   5 Batch    6/78 - Train Accuracy: 0.5440, Validation Accuracy: 0.5457, Loss: 0.9838\n",
      "Epoch   5 Batch    7/78 - Train Accuracy: 0.5453, Validation Accuracy: 0.5507, Loss: 1.0223\n",
      "Epoch   5 Batch    8/78 - Train Accuracy: 0.5244, Validation Accuracy: 0.5530, Loss: 1.0742\n",
      "Epoch   5 Batch    9/78 - Train Accuracy: 0.5219, Validation Accuracy: 0.5470, Loss: 1.0448\n",
      "Epoch   5 Batch   10/78 - Train Accuracy: 0.5192, Validation Accuracy: 0.5463, Loss: 1.0489\n",
      "Epoch   5 Batch   11/78 - Train Accuracy: 0.5261, Validation Accuracy: 0.5525, Loss: 1.0348\n",
      "Epoch   5 Batch   12/78 - Train Accuracy: 0.5100, Validation Accuracy: 0.5549, Loss: 1.0668\n",
      "Epoch   5 Batch   13/78 - Train Accuracy: 0.5544, Validation Accuracy: 0.5457, Loss: 0.9608\n",
      "Epoch   5 Batch   14/78 - Train Accuracy: 0.5277, Validation Accuracy: 0.5441, Loss: 1.0159\n",
      "Epoch   5 Batch   15/78 - Train Accuracy: 0.5235, Validation Accuracy: 0.5477, Loss: 1.0126\n",
      "Epoch   5 Batch   16/78 - Train Accuracy: 0.5492, Validation Accuracy: 0.5510, Loss: 1.0111\n",
      "Epoch   5 Batch   17/78 - Train Accuracy: 0.5311, Validation Accuracy: 0.5466, Loss: 0.9977\n",
      "Epoch   5 Batch   18/78 - Train Accuracy: 0.5162, Validation Accuracy: 0.5490, Loss: 1.0447\n",
      "Epoch   5 Batch   19/78 - Train Accuracy: 0.5654, Validation Accuracy: 0.5530, Loss: 0.9583\n",
      "Epoch   5 Batch   20/78 - Train Accuracy: 0.5301, Validation Accuracy: 0.5552, Loss: 1.0412\n",
      "Epoch   5 Batch   21/78 - Train Accuracy: 0.5249, Validation Accuracy: 0.5518, Loss: 1.0647\n",
      "Epoch   5 Batch   22/78 - Train Accuracy: 0.5417, Validation Accuracy: 0.5539, Loss: 0.9897\n",
      "Epoch   5 Batch   23/78 - Train Accuracy: 0.5445, Validation Accuracy: 0.5563, Loss: 1.0086\n",
      "Epoch   5 Batch   24/78 - Train Accuracy: 0.5302, Validation Accuracy: 0.5548, Loss: 1.0435\n",
      "Epoch   5 Batch   25/78 - Train Accuracy: 0.5217, Validation Accuracy: 0.5526, Loss: 1.0524\n",
      "Epoch   5 Batch   26/78 - Train Accuracy: 0.5676, Validation Accuracy: 0.5549, Loss: 0.9310\n",
      "Epoch   5 Batch   27/78 - Train Accuracy: 0.5354, Validation Accuracy: 0.5554, Loss: 0.9855\n",
      "Epoch   5 Batch   28/78 - Train Accuracy: 0.5038, Validation Accuracy: 0.5543, Loss: 1.0500\n",
      "Epoch   5 Batch   29/78 - Train Accuracy: 0.5121, Validation Accuracy: 0.5522, Loss: 1.0186\n",
      "Epoch   5 Batch   30/78 - Train Accuracy: 0.5447, Validation Accuracy: 0.5516, Loss: 0.9725\n",
      "Epoch   5 Batch   31/78 - Train Accuracy: 0.5533, Validation Accuracy: 0.5584, Loss: 0.9742\n",
      "Epoch   5 Batch   32/78 - Train Accuracy: 0.5398, Validation Accuracy: 0.5597, Loss: 0.9825\n",
      "Epoch   5 Batch   33/78 - Train Accuracy: 0.5505, Validation Accuracy: 0.5583, Loss: 0.9568\n",
      "Epoch   5 Batch   34/78 - Train Accuracy: 0.5578, Validation Accuracy: 0.5616, Loss: 0.9781\n",
      "Epoch   5 Batch   35/78 - Train Accuracy: 0.5593, Validation Accuracy: 0.5605, Loss: 0.9830\n",
      "Epoch   5 Batch   36/78 - Train Accuracy: 0.5402, Validation Accuracy: 0.5603, Loss: 0.9824\n",
      "Epoch   5 Batch   37/78 - Train Accuracy: 0.5379, Validation Accuracy: 0.5582, Loss: 0.9799\n",
      "Epoch   5 Batch   38/78 - Train Accuracy: 0.5339, Validation Accuracy: 0.5544, Loss: 0.9775\n",
      "Epoch   5 Batch   39/78 - Train Accuracy: 0.5464, Validation Accuracy: 0.5566, Loss: 0.9611\n",
      "Epoch   5 Batch   40/78 - Train Accuracy: 0.5190, Validation Accuracy: 0.5555, Loss: 0.9981\n",
      "Epoch   5 Batch   41/78 - Train Accuracy: 0.5414, Validation Accuracy: 0.5562, Loss: 0.9762\n",
      "Epoch   5 Batch   42/78 - Train Accuracy: 0.5660, Validation Accuracy: 0.5553, Loss: 0.9191\n",
      "Epoch   5 Batch   43/78 - Train Accuracy: 0.5223, Validation Accuracy: 0.5583, Loss: 0.9901\n",
      "Epoch   5 Batch   44/78 - Train Accuracy: 0.5589, Validation Accuracy: 0.5613, Loss: 0.9640\n",
      "Epoch   5 Batch   45/78 - Train Accuracy: 0.5252, Validation Accuracy: 0.5613, Loss: 1.0017\n",
      "Epoch   5 Batch   46/78 - Train Accuracy: 0.5371, Validation Accuracy: 0.5589, Loss: 0.9985\n",
      "Epoch   5 Batch   47/78 - Train Accuracy: 0.5706, Validation Accuracy: 0.5596, Loss: 0.8997\n",
      "Epoch   5 Batch   48/78 - Train Accuracy: 0.5544, Validation Accuracy: 0.5596, Loss: 0.9314\n",
      "Epoch   5 Batch   49/78 - Train Accuracy: 0.5365, Validation Accuracy: 0.5634, Loss: 0.9897\n",
      "Epoch   5 Batch   50/78 - Train Accuracy: 0.5351, Validation Accuracy: 0.5604, Loss: 0.9948\n",
      "Epoch   5 Batch   51/78 - Train Accuracy: 0.5417, Validation Accuracy: 0.5602, Loss: 0.9743\n",
      "Epoch   5 Batch   52/78 - Train Accuracy: 0.5380, Validation Accuracy: 0.5582, Loss: 0.9322\n",
      "Epoch   5 Batch   53/78 - Train Accuracy: 0.5276, Validation Accuracy: 0.5613, Loss: 0.9969\n",
      "Epoch   5 Batch   54/78 - Train Accuracy: 0.5444, Validation Accuracy: 0.5644, Loss: 0.9854\n",
      "Epoch   5 Batch   55/78 - Train Accuracy: 0.5507, Validation Accuracy: 0.5614, Loss: 0.9407\n",
      "Epoch   5 Batch   56/78 - Train Accuracy: 0.5697, Validation Accuracy: 0.5662, Loss: 0.9443\n",
      "Epoch   5 Batch   57/78 - Train Accuracy: 0.5596, Validation Accuracy: 0.5664, Loss: 0.9577\n",
      "Epoch   5 Batch   58/78 - Train Accuracy: 0.5560, Validation Accuracy: 0.5637, Loss: 0.9381\n",
      "Epoch   5 Batch   59/78 - Train Accuracy: 0.5546, Validation Accuracy: 0.5622, Loss: 0.9103\n",
      "Epoch   5 Batch   60/78 - Train Accuracy: 0.5534, Validation Accuracy: 0.5611, Loss: 0.8961\n",
      "Epoch   5 Batch   61/78 - Train Accuracy: 0.5711, Validation Accuracy: 0.5664, Loss: 0.8865\n",
      "Epoch   5 Batch   62/78 - Train Accuracy: 0.5715, Validation Accuracy: 0.5672, Loss: 0.9084\n",
      "Epoch   5 Batch   63/78 - Train Accuracy: 0.5388, Validation Accuracy: 0.5654, Loss: 0.9437\n",
      "Epoch   5 Batch   64/78 - Train Accuracy: 0.5535, Validation Accuracy: 0.5686, Loss: 0.9243\n",
      "Epoch   5 Batch   65/78 - Train Accuracy: 0.5571, Validation Accuracy: 0.5672, Loss: 0.9118\n",
      "Epoch   5 Batch   66/78 - Train Accuracy: 0.5597, Validation Accuracy: 0.5682, Loss: 0.8952\n",
      "Epoch   5 Batch   67/78 - Train Accuracy: 0.5452, Validation Accuracy: 0.5686, Loss: 0.9432\n",
      "Epoch   5 Batch   68/78 - Train Accuracy: 0.5392, Validation Accuracy: 0.5636, Loss: 0.9388\n",
      "Epoch   5 Batch   69/78 - Train Accuracy: 0.5326, Validation Accuracy: 0.5631, Loss: 1.0079\n",
      "Epoch   5 Batch   70/78 - Train Accuracy: 0.5597, Validation Accuracy: 0.5679, Loss: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 Batch   71/78 - Train Accuracy: 0.5338, Validation Accuracy: 0.5704, Loss: 0.9654\n",
      "Epoch   5 Batch   72/78 - Train Accuracy: 0.5673, Validation Accuracy: 0.5692, Loss: 0.9018\n",
      "Epoch   5 Batch   73/78 - Train Accuracy: 0.5491, Validation Accuracy: 0.5685, Loss: 0.9496\n",
      "Epoch   5 Batch   74/78 - Train Accuracy: 0.5503, Validation Accuracy: 0.5679, Loss: 0.9345\n",
      "Epoch   5 Batch   75/78 - Train Accuracy: 0.5548, Validation Accuracy: 0.5661, Loss: 0.9129\n",
      "Epoch   5 Batch   76/78 - Train Accuracy: 0.5408, Validation Accuracy: 0.5662, Loss: 0.9445\n",
      "Epoch   6 Batch    1/78 - Train Accuracy: 0.5343, Validation Accuracy: 0.5670, Loss: 0.9366\n",
      "Epoch   6 Batch    2/78 - Train Accuracy: 0.5318, Validation Accuracy: 0.5678, Loss: 0.9265\n",
      "Epoch   6 Batch    3/78 - Train Accuracy: 0.5453, Validation Accuracy: 0.5676, Loss: 0.9426\n",
      "Epoch   6 Batch    4/78 - Train Accuracy: 0.5373, Validation Accuracy: 0.5661, Loss: 0.9467\n",
      "Epoch   6 Batch    5/78 - Train Accuracy: 0.5261, Validation Accuracy: 0.5673, Loss: 0.9553\n",
      "Epoch   6 Batch    6/78 - Train Accuracy: 0.5560, Validation Accuracy: 0.5685, Loss: 0.8627\n",
      "Epoch   6 Batch    7/78 - Train Accuracy: 0.5513, Validation Accuracy: 0.5670, Loss: 0.8965\n",
      "Epoch   6 Batch    8/78 - Train Accuracy: 0.5282, Validation Accuracy: 0.5663, Loss: 0.9558\n",
      "Epoch   6 Batch    9/78 - Train Accuracy: 0.5370, Validation Accuracy: 0.5649, Loss: 0.9118\n",
      "Epoch   6 Batch   10/78 - Train Accuracy: 0.5428, Validation Accuracy: 0.5673, Loss: 0.9194\n",
      "Epoch   6 Batch   11/78 - Train Accuracy: 0.5458, Validation Accuracy: 0.5693, Loss: 0.9124\n",
      "Epoch   6 Batch   12/78 - Train Accuracy: 0.5229, Validation Accuracy: 0.5694, Loss: 0.9394\n",
      "Epoch   6 Batch   13/78 - Train Accuracy: 0.5855, Validation Accuracy: 0.5701, Loss: 0.8414\n",
      "Epoch   6 Batch   14/78 - Train Accuracy: 0.5524, Validation Accuracy: 0.5691, Loss: 0.8940\n",
      "Epoch   6 Batch   15/78 - Train Accuracy: 0.5439, Validation Accuracy: 0.5648, Loss: 0.8853\n",
      "Epoch   6 Batch   16/78 - Train Accuracy: 0.5747, Validation Accuracy: 0.5690, Loss: 0.8897\n",
      "Epoch   6 Batch   17/78 - Train Accuracy: 0.5526, Validation Accuracy: 0.5646, Loss: 0.8718\n",
      "Epoch   6 Batch   18/78 - Train Accuracy: 0.5320, Validation Accuracy: 0.5659, Loss: 0.9151\n",
      "Epoch   6 Batch   19/78 - Train Accuracy: 0.5801, Validation Accuracy: 0.5643, Loss: 0.8449\n",
      "Epoch   6 Batch   20/78 - Train Accuracy: 0.5420, Validation Accuracy: 0.5682, Loss: 0.9242\n",
      "Epoch   6 Batch   21/78 - Train Accuracy: 0.5438, Validation Accuracy: 0.5670, Loss: 0.9468\n",
      "Epoch   6 Batch   22/78 - Train Accuracy: 0.5556, Validation Accuracy: 0.5648, Loss: 0.8737\n",
      "Epoch   6 Batch   23/78 - Train Accuracy: 0.5667, Validation Accuracy: 0.5635, Loss: 0.9050\n",
      "Epoch   6 Batch   24/78 - Train Accuracy: 0.5403, Validation Accuracy: 0.5699, Loss: 0.9193\n",
      "Epoch   6 Batch   25/78 - Train Accuracy: 0.5384, Validation Accuracy: 0.5749, Loss: 0.9301\n",
      "Epoch   6 Batch   26/78 - Train Accuracy: 0.5843, Validation Accuracy: 0.5695, Loss: 0.8283\n",
      "Epoch   6 Batch   27/78 - Train Accuracy: 0.5605, Validation Accuracy: 0.5708, Loss: 0.8740\n",
      "Epoch   6 Batch   28/78 - Train Accuracy: 0.5238, Validation Accuracy: 0.5708, Loss: 0.9344\n",
      "Epoch   6 Batch   29/78 - Train Accuracy: 0.5388, Validation Accuracy: 0.5701, Loss: 0.8974\n",
      "Epoch   6 Batch   30/78 - Train Accuracy: 0.5617, Validation Accuracy: 0.5682, Loss: 0.8670\n",
      "Epoch   6 Batch   31/78 - Train Accuracy: 0.5661, Validation Accuracy: 0.5679, Loss: 0.8578\n",
      "Epoch   6 Batch   32/78 - Train Accuracy: 0.5441, Validation Accuracy: 0.5710, Loss: 0.8714\n",
      "Epoch   6 Batch   33/78 - Train Accuracy: 0.5558, Validation Accuracy: 0.5706, Loss: 0.8492\n",
      "Epoch   6 Batch   34/78 - Train Accuracy: 0.5572, Validation Accuracy: 0.5657, Loss: 0.8700\n",
      "Epoch   6 Batch   35/78 - Train Accuracy: 0.5684, Validation Accuracy: 0.5627, Loss: 0.8735\n",
      "Epoch   6 Batch   36/78 - Train Accuracy: 0.5525, Validation Accuracy: 0.5654, Loss: 0.8707\n",
      "Epoch   6 Batch   37/78 - Train Accuracy: 0.5511, Validation Accuracy: 0.5704, Loss: 0.8722\n",
      "Epoch   6 Batch   38/78 - Train Accuracy: 0.5535, Validation Accuracy: 0.5709, Loss: 0.8727\n",
      "Epoch   6 Batch   39/78 - Train Accuracy: 0.5701, Validation Accuracy: 0.5696, Loss: 0.8490\n",
      "Epoch   6 Batch   40/78 - Train Accuracy: 0.5485, Validation Accuracy: 0.5732, Loss: 0.8913\n",
      "Epoch   6 Batch   41/78 - Train Accuracy: 0.5645, Validation Accuracy: 0.5763, Loss: 0.8792\n",
      "Epoch   6 Batch   42/78 - Train Accuracy: 0.6045, Validation Accuracy: 0.5889, Loss: 0.8225\n",
      "Epoch   6 Batch   43/78 - Train Accuracy: 0.5645, Validation Accuracy: 0.5888, Loss: 0.8895\n",
      "Epoch   6 Batch   44/78 - Train Accuracy: 0.5913, Validation Accuracy: 0.5889, Loss: 0.8635\n",
      "Epoch   6 Batch   45/78 - Train Accuracy: 0.5608, Validation Accuracy: 0.5828, Loss: 0.8983\n",
      "Epoch   6 Batch   46/78 - Train Accuracy: 0.5658, Validation Accuracy: 0.5852, Loss: 0.8877\n",
      "Epoch   6 Batch   47/78 - Train Accuracy: 0.6003, Validation Accuracy: 0.5833, Loss: 0.8014\n",
      "Epoch   6 Batch   48/78 - Train Accuracy: 0.5830, Validation Accuracy: 0.5826, Loss: 0.8351\n",
      "Epoch   6 Batch   49/78 - Train Accuracy: 0.5505, Validation Accuracy: 0.5860, Loss: 0.8790\n",
      "Epoch   6 Batch   50/78 - Train Accuracy: 0.5577, Validation Accuracy: 0.5821, Loss: 0.8885\n",
      "Epoch   6 Batch   51/78 - Train Accuracy: 0.5621, Validation Accuracy: 0.5840, Loss: 0.8631\n",
      "Epoch   6 Batch   52/78 - Train Accuracy: 0.5652, Validation Accuracy: 0.5835, Loss: 0.8239\n",
      "Epoch   6 Batch   53/78 - Train Accuracy: 0.5556, Validation Accuracy: 0.5866, Loss: 0.8929\n",
      "Epoch   6 Batch   54/78 - Train Accuracy: 0.5744, Validation Accuracy: 0.5889, Loss: 0.8778\n",
      "Epoch   6 Batch   55/78 - Train Accuracy: 0.5876, Validation Accuracy: 0.5839, Loss: 0.8389\n",
      "Epoch   6 Batch   56/78 - Train Accuracy: 0.5908, Validation Accuracy: 0.5848, Loss: 0.8502\n",
      "Epoch   6 Batch   57/78 - Train Accuracy: 0.5851, Validation Accuracy: 0.5877, Loss: 0.8589\n",
      "Epoch   6 Batch   58/78 - Train Accuracy: 0.5875, Validation Accuracy: 0.5923, Loss: 0.8375\n",
      "Epoch   6 Batch   59/78 - Train Accuracy: 0.5938, Validation Accuracy: 0.5934, Loss: 0.8197\n",
      "Epoch   6 Batch   60/78 - Train Accuracy: 0.5978, Validation Accuracy: 0.5953, Loss: 0.8100\n",
      "Epoch   6 Batch   61/78 - Train Accuracy: 0.5986, Validation Accuracy: 0.5925, Loss: 0.7944\n",
      "Epoch   6 Batch   62/78 - Train Accuracy: 0.5960, Validation Accuracy: 0.5918, Loss: 0.8231\n",
      "Epoch   6 Batch   63/78 - Train Accuracy: 0.5692, Validation Accuracy: 0.5906, Loss: 0.8484\n",
      "Epoch   6 Batch   64/78 - Train Accuracy: 0.5725, Validation Accuracy: 0.5898, Loss: 0.8273\n",
      "Epoch   6 Batch   65/78 - Train Accuracy: 0.5826, Validation Accuracy: 0.5895, Loss: 0.8256\n",
      "Epoch   6 Batch   66/78 - Train Accuracy: 0.5843, Validation Accuracy: 0.5910, Loss: 0.8117\n",
      "Epoch   6 Batch   67/78 - Train Accuracy: 0.5901, Validation Accuracy: 0.5957, Loss: 0.8385\n",
      "Epoch   6 Batch   68/78 - Train Accuracy: 0.5800, Validation Accuracy: 0.5917, Loss: 0.8503\n",
      "Epoch   6 Batch   69/78 - Train Accuracy: 0.5590, Validation Accuracy: 0.5868, Loss: 0.9111\n",
      "Epoch   6 Batch   70/78 - Train Accuracy: 0.5963, Validation Accuracy: 0.5886, Loss: 0.8347\n",
      "Epoch   6 Batch   71/78 - Train Accuracy: 0.5720, Validation Accuracy: 0.5938, Loss: 0.8763\n",
      "Epoch   6 Batch   72/78 - Train Accuracy: 0.5945, Validation Accuracy: 0.5929, Loss: 0.8214\n",
      "Epoch   6 Batch   73/78 - Train Accuracy: 0.5833, Validation Accuracy: 0.5895, Loss: 0.8512\n",
      "Epoch   6 Batch   74/78 - Train Accuracy: 0.5754, Validation Accuracy: 0.5882, Loss: 0.8529\n",
      "Epoch   6 Batch   75/78 - Train Accuracy: 0.5789, Validation Accuracy: 0.5909, Loss: 0.8282\n",
      "Epoch   6 Batch   76/78 - Train Accuracy: 0.5711, Validation Accuracy: 0.5877, Loss: 0.8457\n",
      "Epoch   7 Batch    1/78 - Train Accuracy: 0.5712, Validation Accuracy: 0.5954, Loss: 0.8476\n",
      "Epoch   7 Batch    2/78 - Train Accuracy: 0.5668, Validation Accuracy: 0.5904, Loss: 0.8440\n",
      "Epoch   7 Batch    3/78 - Train Accuracy: 0.5787, Validation Accuracy: 0.5887, Loss: 0.8553\n",
      "Epoch   7 Batch    4/78 - Train Accuracy: 0.5601, Validation Accuracy: 0.5954, Loss: 0.8598\n",
      "Epoch   7 Batch    5/78 - Train Accuracy: 0.5666, Validation Accuracy: 0.5997, Loss: 0.8628\n",
      "Epoch   7 Batch    6/78 - Train Accuracy: 0.5939, Validation Accuracy: 0.5988, Loss: 0.7883\n",
      "Epoch   7 Batch    7/78 - Train Accuracy: 0.5919, Validation Accuracy: 0.5938, Loss: 0.8101\n",
      "Epoch   7 Batch    8/78 - Train Accuracy: 0.5582, Validation Accuracy: 0.5888, Loss: 0.8590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7 Batch    9/78 - Train Accuracy: 0.5809, Validation Accuracy: 0.5981, Loss: 0.8327\n",
      "Epoch   7 Batch   10/78 - Train Accuracy: 0.5778, Validation Accuracy: 0.5991, Loss: 0.8318\n",
      "Epoch   7 Batch   11/78 - Train Accuracy: 0.5789, Validation Accuracy: 0.6006, Loss: 0.8316\n",
      "Epoch   7 Batch   12/78 - Train Accuracy: 0.5648, Validation Accuracy: 0.6032, Loss: 0.8537\n",
      "Epoch   7 Batch   13/78 - Train Accuracy: 0.6112, Validation Accuracy: 0.5926, Loss: 0.7678\n",
      "Epoch   7 Batch   14/78 - Train Accuracy: 0.5831, Validation Accuracy: 0.5912, Loss: 0.8159\n",
      "Epoch   7 Batch   15/78 - Train Accuracy: 0.5785, Validation Accuracy: 0.5971, Loss: 0.8059\n",
      "Epoch   7 Batch   16/78 - Train Accuracy: 0.5987, Validation Accuracy: 0.6007, Loss: 0.8133\n",
      "Epoch   7 Batch   17/78 - Train Accuracy: 0.5928, Validation Accuracy: 0.6021, Loss: 0.7990\n",
      "Epoch   7 Batch   18/78 - Train Accuracy: 0.5615, Validation Accuracy: 0.5963, Loss: 0.8307\n",
      "Epoch   7 Batch   19/78 - Train Accuracy: 0.6112, Validation Accuracy: 0.5969, Loss: 0.7611\n",
      "Epoch   7 Batch   20/78 - Train Accuracy: 0.5890, Validation Accuracy: 0.6026, Loss: 0.8437\n",
      "Epoch   7 Batch   21/78 - Train Accuracy: 0.5809, Validation Accuracy: 0.6013, Loss: 0.8613\n",
      "Epoch   7 Batch   22/78 - Train Accuracy: 0.5910, Validation Accuracy: 0.5993, Loss: 0.7942\n",
      "Epoch   7 Batch   23/78 - Train Accuracy: 0.5958, Validation Accuracy: 0.5986, Loss: 0.8188\n",
      "Epoch   7 Batch   24/78 - Train Accuracy: 0.5776, Validation Accuracy: 0.6004, Loss: 0.8460\n",
      "Epoch   7 Batch   25/78 - Train Accuracy: 0.5692, Validation Accuracy: 0.5993, Loss: 0.8534\n",
      "Epoch   7 Batch   26/78 - Train Accuracy: 0.6149, Validation Accuracy: 0.5952, Loss: 0.7549\n",
      "Epoch   7 Batch   27/78 - Train Accuracy: 0.5776, Validation Accuracy: 0.5956, Loss: 0.8043\n",
      "Epoch   7 Batch   28/78 - Train Accuracy: 0.5533, Validation Accuracy: 0.6012, Loss: 0.8495\n",
      "Epoch   7 Batch   29/78 - Train Accuracy: 0.5814, Validation Accuracy: 0.6002, Loss: 0.8262\n",
      "Epoch   7 Batch   30/78 - Train Accuracy: 0.5828, Validation Accuracy: 0.5937, Loss: 0.7959\n",
      "Epoch   7 Batch   31/78 - Train Accuracy: 0.5928, Validation Accuracy: 0.5902, Loss: 0.7841\n",
      "Epoch   7 Batch   32/78 - Train Accuracy: 0.5806, Validation Accuracy: 0.5934, Loss: 0.8011\n",
      "Epoch   7 Batch   33/78 - Train Accuracy: 0.6090, Validation Accuracy: 0.6009, Loss: 0.7749\n",
      "Epoch   7 Batch   34/78 - Train Accuracy: 0.5976, Validation Accuracy: 0.6030, Loss: 0.7903\n",
      "Epoch   7 Batch   35/78 - Train Accuracy: 0.5936, Validation Accuracy: 0.6023, Loss: 0.8070\n",
      "Epoch   7 Batch   36/78 - Train Accuracy: 0.5924, Validation Accuracy: 0.6038, Loss: 0.7994\n",
      "Epoch   7 Batch   37/78 - Train Accuracy: 0.6027, Validation Accuracy: 0.6062, Loss: 0.7934\n",
      "Epoch   7 Batch   38/78 - Train Accuracy: 0.5969, Validation Accuracy: 0.6043, Loss: 0.8078\n",
      "Epoch   7 Batch   39/78 - Train Accuracy: 0.6003, Validation Accuracy: 0.6020, Loss: 0.7859\n",
      "Epoch   7 Batch   40/78 - Train Accuracy: 0.5858, Validation Accuracy: 0.6040, Loss: 0.8239\n",
      "Epoch   7 Batch   41/78 - Train Accuracy: 0.5922, Validation Accuracy: 0.6030, Loss: 0.8016\n",
      "Epoch   7 Batch   42/78 - Train Accuracy: 0.6188, Validation Accuracy: 0.5985, Loss: 0.7569\n",
      "Epoch   7 Batch   43/78 - Train Accuracy: 0.5763, Validation Accuracy: 0.6007, Loss: 0.8115\n",
      "Epoch   7 Batch   44/78 - Train Accuracy: 0.6055, Validation Accuracy: 0.6064, Loss: 0.7940\n",
      "Epoch   7 Batch   45/78 - Train Accuracy: 0.5750, Validation Accuracy: 0.6048, Loss: 0.8252\n",
      "Epoch   7 Batch   46/78 - Train Accuracy: 0.5877, Validation Accuracy: 0.6017, Loss: 0.8081\n",
      "Epoch   7 Batch   47/78 - Train Accuracy: 0.6174, Validation Accuracy: 0.6011, Loss: 0.7360\n",
      "Epoch   7 Batch   48/78 - Train Accuracy: 0.5948, Validation Accuracy: 0.6032, Loss: 0.7705\n",
      "Epoch   7 Batch   49/78 - Train Accuracy: 0.5709, Validation Accuracy: 0.6056, Loss: 0.8060\n",
      "Epoch   7 Batch   50/78 - Train Accuracy: 0.5763, Validation Accuracy: 0.6016, Loss: 0.8172\n",
      "Epoch   7 Batch   51/78 - Train Accuracy: 0.5877, Validation Accuracy: 0.6038, Loss: 0.7909\n",
      "Epoch   7 Batch   52/78 - Train Accuracy: 0.5919, Validation Accuracy: 0.6076, Loss: 0.7613\n",
      "Epoch   7 Batch   53/78 - Train Accuracy: 0.5771, Validation Accuracy: 0.6072, Loss: 0.8238\n",
      "Epoch   7 Batch   54/78 - Train Accuracy: 0.5884, Validation Accuracy: 0.6040, Loss: 0.8035\n",
      "Epoch   7 Batch   55/78 - Train Accuracy: 0.6090, Validation Accuracy: 0.6017, Loss: 0.7757\n",
      "Epoch   7 Batch   56/78 - Train Accuracy: 0.6152, Validation Accuracy: 0.6066, Loss: 0.7859\n",
      "Epoch   7 Batch   57/78 - Train Accuracy: 0.5973, Validation Accuracy: 0.6056, Loss: 0.7934\n",
      "Epoch   7 Batch   58/78 - Train Accuracy: 0.6063, Validation Accuracy: 0.6025, Loss: 0.7706\n",
      "Epoch   7 Batch   59/78 - Train Accuracy: 0.6124, Validation Accuracy: 0.6048, Loss: 0.7548\n",
      "Epoch   7 Batch   60/78 - Train Accuracy: 0.6107, Validation Accuracy: 0.6052, Loss: 0.7444\n",
      "Epoch   7 Batch   61/78 - Train Accuracy: 0.6115, Validation Accuracy: 0.6017, Loss: 0.7299\n",
      "Epoch   7 Batch   62/78 - Train Accuracy: 0.6092, Validation Accuracy: 0.6062, Loss: 0.7549\n",
      "Epoch   7 Batch   63/78 - Train Accuracy: 0.5877, Validation Accuracy: 0.6057, Loss: 0.7836\n",
      "Epoch   7 Batch   64/78 - Train Accuracy: 0.5928, Validation Accuracy: 0.6087, Loss: 0.7699\n",
      "Epoch   7 Batch   65/78 - Train Accuracy: 0.5981, Validation Accuracy: 0.6047, Loss: 0.7586\n",
      "Epoch   7 Batch   66/78 - Train Accuracy: 0.6035, Validation Accuracy: 0.6036, Loss: 0.7451\n",
      "Epoch   7 Batch   67/78 - Train Accuracy: 0.5971, Validation Accuracy: 0.6069, Loss: 0.7818\n",
      "Epoch   7 Batch   68/78 - Train Accuracy: 0.5837, Validation Accuracy: 0.6045, Loss: 0.7827\n",
      "Epoch   7 Batch   69/78 - Train Accuracy: 0.5713, Validation Accuracy: 0.6003, Loss: 0.8418\n",
      "Epoch   7 Batch   70/78 - Train Accuracy: 0.6050, Validation Accuracy: 0.5995, Loss: 0.7690\n",
      "Epoch   7 Batch   71/78 - Train Accuracy: 0.5799, Validation Accuracy: 0.6072, Loss: 0.8076\n",
      "Epoch   7 Batch   72/78 - Train Accuracy: 0.6087, Validation Accuracy: 0.6077, Loss: 0.7636\n",
      "Epoch   7 Batch   73/78 - Train Accuracy: 0.5953, Validation Accuracy: 0.6080, Loss: 0.7871\n",
      "Epoch   7 Batch   74/78 - Train Accuracy: 0.6000, Validation Accuracy: 0.6078, Loss: 0.7816\n",
      "Epoch   7 Batch   75/78 - Train Accuracy: 0.5951, Validation Accuracy: 0.6068, Loss: 0.7674\n",
      "Epoch   7 Batch   76/78 - Train Accuracy: 0.5900, Validation Accuracy: 0.6109, Loss: 0.7815\n",
      "Epoch   8 Batch    1/78 - Train Accuracy: 0.5853, Validation Accuracy: 0.6108, Loss: 0.7831\n",
      "Epoch   8 Batch    2/78 - Train Accuracy: 0.5806, Validation Accuracy: 0.6079, Loss: 0.7771\n",
      "Epoch   8 Batch    3/78 - Train Accuracy: 0.6007, Validation Accuracy: 0.6070, Loss: 0.7841\n",
      "Epoch   8 Batch    4/78 - Train Accuracy: 0.5711, Validation Accuracy: 0.6033, Loss: 0.7970\n",
      "Epoch   8 Batch    5/78 - Train Accuracy: 0.5737, Validation Accuracy: 0.6091, Loss: 0.7970\n",
      "Epoch   8 Batch    6/78 - Train Accuracy: 0.6079, Validation Accuracy: 0.6078, Loss: 0.7272\n",
      "Epoch   8 Batch    7/78 - Train Accuracy: 0.6066, Validation Accuracy: 0.6078, Loss: 0.7525\n",
      "Epoch   8 Batch    8/78 - Train Accuracy: 0.5834, Validation Accuracy: 0.6053, Loss: 0.7975\n",
      "Epoch   8 Batch    9/78 - Train Accuracy: 0.5919, Validation Accuracy: 0.6045, Loss: 0.7754\n",
      "Epoch   8 Batch   10/78 - Train Accuracy: 0.5913, Validation Accuracy: 0.6064, Loss: 0.7805\n",
      "Epoch   8 Batch   11/78 - Train Accuracy: 0.5968, Validation Accuracy: 0.6119, Loss: 0.7726\n",
      "Epoch   8 Batch   12/78 - Train Accuracy: 0.5756, Validation Accuracy: 0.6122, Loss: 0.7946\n",
      "Epoch   8 Batch   13/78 - Train Accuracy: 0.6180, Validation Accuracy: 0.6094, Loss: 0.7076\n",
      "Epoch   8 Batch   14/78 - Train Accuracy: 0.5957, Validation Accuracy: 0.6110, Loss: 0.7601\n",
      "Epoch   8 Batch   15/78 - Train Accuracy: 0.5960, Validation Accuracy: 0.6179, Loss: 0.7471\n",
      "Epoch   8 Batch   16/78 - Train Accuracy: 0.6154, Validation Accuracy: 0.6160, Loss: 0.7518\n",
      "Epoch   8 Batch   17/78 - Train Accuracy: 0.6025, Validation Accuracy: 0.6121, Loss: 0.7387\n",
      "Epoch   8 Batch   18/78 - Train Accuracy: 0.5768, Validation Accuracy: 0.6143, Loss: 0.7718\n",
      "Epoch   8 Batch   19/78 - Train Accuracy: 0.6313, Validation Accuracy: 0.6168, Loss: 0.7098\n",
      "Epoch   8 Batch   20/78 - Train Accuracy: 0.5973, Validation Accuracy: 0.6140, Loss: 0.7751\n",
      "Epoch   8 Batch   21/78 - Train Accuracy: 0.5970, Validation Accuracy: 0.6127, Loss: 0.7944\n",
      "Epoch   8 Batch   22/78 - Train Accuracy: 0.6053, Validation Accuracy: 0.6115, Loss: 0.7267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8 Batch   23/78 - Train Accuracy: 0.6097, Validation Accuracy: 0.6112, Loss: 0.7536\n",
      "Epoch   8 Batch   24/78 - Train Accuracy: 0.5934, Validation Accuracy: 0.6164, Loss: 0.7844\n",
      "Epoch   8 Batch   25/78 - Train Accuracy: 0.5890, Validation Accuracy: 0.6152, Loss: 0.7954\n",
      "Epoch   8 Batch   26/78 - Train Accuracy: 0.6289, Validation Accuracy: 0.6157, Loss: 0.6999\n",
      "Epoch   8 Batch   27/78 - Train Accuracy: 0.6007, Validation Accuracy: 0.6151, Loss: 0.7420\n",
      "Epoch   8 Batch   28/78 - Train Accuracy: 0.5646, Validation Accuracy: 0.6167, Loss: 0.7968\n",
      "Epoch   8 Batch   29/78 - Train Accuracy: 0.5979, Validation Accuracy: 0.6167, Loss: 0.7653\n",
      "Epoch   8 Batch   30/78 - Train Accuracy: 0.6063, Validation Accuracy: 0.6171, Loss: 0.7384\n",
      "Epoch   8 Batch   31/78 - Train Accuracy: 0.6158, Validation Accuracy: 0.6144, Loss: 0.7308\n",
      "Epoch   8 Batch   32/78 - Train Accuracy: 0.5904, Validation Accuracy: 0.6102, Loss: 0.7390\n",
      "Epoch   8 Batch   33/78 - Train Accuracy: 0.6246, Validation Accuracy: 0.6171, Loss: 0.7192\n",
      "Epoch   8 Batch   34/78 - Train Accuracy: 0.6128, Validation Accuracy: 0.6164, Loss: 0.7353\n",
      "Epoch   8 Batch   35/78 - Train Accuracy: 0.6081, Validation Accuracy: 0.6084, Loss: 0.7543\n",
      "Epoch   8 Batch   36/78 - Train Accuracy: 0.6036, Validation Accuracy: 0.6099, Loss: 0.7384\n",
      "Epoch   8 Batch   37/78 - Train Accuracy: 0.6192, Validation Accuracy: 0.6157, Loss: 0.7345\n",
      "Epoch   8 Batch   38/78 - Train Accuracy: 0.6088, Validation Accuracy: 0.6136, Loss: 0.7380\n",
      "Epoch   8 Batch   39/78 - Train Accuracy: 0.6104, Validation Accuracy: 0.6150, Loss: 0.7348\n",
      "Epoch   8 Batch   40/78 - Train Accuracy: 0.5908, Validation Accuracy: 0.6092, Loss: 0.7669\n",
      "Epoch   8 Batch   41/78 - Train Accuracy: 0.5982, Validation Accuracy: 0.6112, Loss: 0.7546\n",
      "Epoch   8 Batch   42/78 - Train Accuracy: 0.6334, Validation Accuracy: 0.6119, Loss: 0.6996\n",
      "Epoch   8 Batch   43/78 - Train Accuracy: 0.5976, Validation Accuracy: 0.6175, Loss: 0.7633\n",
      "Epoch   8 Batch   44/78 - Train Accuracy: 0.6160, Validation Accuracy: 0.6135, Loss: 0.7374\n",
      "Epoch   8 Batch   45/78 - Train Accuracy: 0.5881, Validation Accuracy: 0.6079, Loss: 0.7685\n",
      "Epoch   8 Batch   46/78 - Train Accuracy: 0.6014, Validation Accuracy: 0.6104, Loss: 0.7560\n",
      "Epoch   8 Batch   47/78 - Train Accuracy: 0.6337, Validation Accuracy: 0.6137, Loss: 0.6841\n",
      "Epoch   8 Batch   48/78 - Train Accuracy: 0.6023, Validation Accuracy: 0.6100, Loss: 0.7094\n",
      "Epoch   8 Batch   49/78 - Train Accuracy: 0.5828, Validation Accuracy: 0.6081, Loss: 0.7496\n",
      "Epoch   8 Batch   50/78 - Train Accuracy: 0.5929, Validation Accuracy: 0.6129, Loss: 0.7656\n",
      "Epoch   8 Batch   51/78 - Train Accuracy: 0.6029, Validation Accuracy: 0.6139, Loss: 0.7377\n",
      "Epoch   8 Batch   52/78 - Train Accuracy: 0.6021, Validation Accuracy: 0.6112, Loss: 0.7146\n",
      "Epoch   8 Batch   53/78 - Train Accuracy: 0.5852, Validation Accuracy: 0.6079, Loss: 0.7683\n",
      "Epoch   8 Batch   54/78 - Train Accuracy: 0.6014, Validation Accuracy: 0.6147, Loss: 0.7469\n",
      "Epoch   8 Batch   55/78 - Train Accuracy: 0.6217, Validation Accuracy: 0.6150, Loss: 0.7191\n",
      "Epoch   8 Batch   56/78 - Train Accuracy: 0.6230, Validation Accuracy: 0.6137, Loss: 0.7373\n",
      "Epoch   8 Batch   57/78 - Train Accuracy: 0.6067, Validation Accuracy: 0.6136, Loss: 0.7364\n",
      "Epoch   8 Batch   58/78 - Train Accuracy: 0.6169, Validation Accuracy: 0.6159, Loss: 0.7168\n",
      "Epoch   8 Batch   59/78 - Train Accuracy: 0.6169, Validation Accuracy: 0.6137, Loss: 0.6989\n",
      "Epoch   8 Batch   60/78 - Train Accuracy: 0.6156, Validation Accuracy: 0.6100, Loss: 0.6950\n",
      "Epoch   8 Batch   61/78 - Train Accuracy: 0.6250, Validation Accuracy: 0.6158, Loss: 0.6754\n",
      "Epoch   8 Batch   62/78 - Train Accuracy: 0.6211, Validation Accuracy: 0.6179, Loss: 0.7020\n",
      "Epoch   8 Batch   63/78 - Train Accuracy: 0.6008, Validation Accuracy: 0.6126, Loss: 0.7333\n",
      "Epoch   8 Batch   64/78 - Train Accuracy: 0.6004, Validation Accuracy: 0.6120, Loss: 0.7160\n",
      "Epoch   8 Batch   65/78 - Train Accuracy: 0.6100, Validation Accuracy: 0.6147, Loss: 0.7101\n",
      "Epoch   8 Batch   66/78 - Train Accuracy: 0.6172, Validation Accuracy: 0.6140, Loss: 0.7004\n",
      "Epoch   8 Batch   67/78 - Train Accuracy: 0.6092, Validation Accuracy: 0.6134, Loss: 0.7282\n",
      "Epoch   8 Batch   68/78 - Train Accuracy: 0.5979, Validation Accuracy: 0.6114, Loss: 0.7253\n",
      "Epoch   8 Batch   69/78 - Train Accuracy: 0.5864, Validation Accuracy: 0.6143, Loss: 0.7868\n",
      "Epoch   8 Batch   70/78 - Train Accuracy: 0.6200, Validation Accuracy: 0.6127, Loss: 0.7165\n",
      "Epoch   8 Batch   71/78 - Train Accuracy: 0.5976, Validation Accuracy: 0.6141, Loss: 0.7522\n",
      "Epoch   8 Batch   72/78 - Train Accuracy: 0.6212, Validation Accuracy: 0.6126, Loss: 0.7081\n",
      "Epoch   8 Batch   73/78 - Train Accuracy: 0.6061, Validation Accuracy: 0.6130, Loss: 0.7324\n",
      "Epoch   8 Batch   74/78 - Train Accuracy: 0.6117, Validation Accuracy: 0.6154, Loss: 0.7293\n",
      "Epoch   8 Batch   75/78 - Train Accuracy: 0.6088, Validation Accuracy: 0.6157, Loss: 0.7093\n",
      "Epoch   8 Batch   76/78 - Train Accuracy: 0.6010, Validation Accuracy: 0.6151, Loss: 0.7238\n",
      "Epoch   9 Batch    1/78 - Train Accuracy: 0.5920, Validation Accuracy: 0.6164, Loss: 0.7294\n",
      "Epoch   9 Batch    2/78 - Train Accuracy: 0.5865, Validation Accuracy: 0.6163, Loss: 0.7284\n",
      "Epoch   9 Batch    3/78 - Train Accuracy: 0.6064, Validation Accuracy: 0.6167, Loss: 0.7322\n",
      "Epoch   9 Batch    4/78 - Train Accuracy: 0.5806, Validation Accuracy: 0.6159, Loss: 0.7415\n",
      "Epoch   9 Batch    5/78 - Train Accuracy: 0.5838, Validation Accuracy: 0.6142, Loss: 0.7416\n",
      "Epoch   9 Batch    6/78 - Train Accuracy: 0.6151, Validation Accuracy: 0.6140, Loss: 0.6891\n",
      "Epoch   9 Batch    7/78 - Train Accuracy: 0.6120, Validation Accuracy: 0.6150, Loss: 0.6982\n",
      "Epoch   9 Batch    8/78 - Train Accuracy: 0.5920, Validation Accuracy: 0.6167, Loss: 0.7494\n",
      "Epoch   9 Batch    9/78 - Train Accuracy: 0.6010, Validation Accuracy: 0.6149, Loss: 0.7279\n",
      "Epoch   9 Batch   10/78 - Train Accuracy: 0.5938, Validation Accuracy: 0.6142, Loss: 0.7236\n",
      "Epoch   9 Batch   11/78 - Train Accuracy: 0.6084, Validation Accuracy: 0.6154, Loss: 0.7196\n",
      "Epoch   9 Batch   12/78 - Train Accuracy: 0.5864, Validation Accuracy: 0.6167, Loss: 0.7462\n",
      "Epoch   9 Batch   13/78 - Train Accuracy: 0.6284, Validation Accuracy: 0.6138, Loss: 0.6590\n",
      "Epoch   9 Batch   14/78 - Train Accuracy: 0.6049, Validation Accuracy: 0.6127, Loss: 0.7133\n",
      "Epoch   9 Batch   15/78 - Train Accuracy: 0.5947, Validation Accuracy: 0.6105, Loss: 0.6938\n",
      "Epoch   9 Batch   16/78 - Train Accuracy: 0.6187, Validation Accuracy: 0.6174, Loss: 0.7037\n",
      "Epoch   9 Batch   17/78 - Train Accuracy: 0.6155, Validation Accuracy: 0.6168, Loss: 0.6909\n",
      "Epoch   9 Batch   18/78 - Train Accuracy: 0.5853, Validation Accuracy: 0.6148, Loss: 0.7214\n",
      "Epoch   9 Batch   19/78 - Train Accuracy: 0.6334, Validation Accuracy: 0.6102, Loss: 0.6592\n",
      "Epoch   9 Batch   20/78 - Train Accuracy: 0.5971, Validation Accuracy: 0.6136, Loss: 0.7321\n",
      "Epoch   9 Batch   21/78 - Train Accuracy: 0.6024, Validation Accuracy: 0.6183, Loss: 0.7437\n",
      "Epoch   9 Batch   22/78 - Train Accuracy: 0.6126, Validation Accuracy: 0.6181, Loss: 0.6859\n",
      "Epoch   9 Batch   23/78 - Train Accuracy: 0.6148, Validation Accuracy: 0.6127, Loss: 0.6978\n",
      "Epoch   9 Batch   24/78 - Train Accuracy: 0.5991, Validation Accuracy: 0.6131, Loss: 0.7341\n",
      "Epoch   9 Batch   25/78 - Train Accuracy: 0.5969, Validation Accuracy: 0.6172, Loss: 0.7432\n",
      "Epoch   9 Batch   26/78 - Train Accuracy: 0.6325, Validation Accuracy: 0.6141, Loss: 0.6503\n",
      "Epoch   9 Batch   27/78 - Train Accuracy: 0.6087, Validation Accuracy: 0.6182, Loss: 0.6904\n",
      "Epoch   9 Batch   28/78 - Train Accuracy: 0.5763, Validation Accuracy: 0.6205, Loss: 0.7442\n",
      "Epoch   9 Batch   29/78 - Train Accuracy: 0.6001, Validation Accuracy: 0.6183, Loss: 0.7167\n",
      "Epoch   9 Batch   30/78 - Train Accuracy: 0.6118, Validation Accuracy: 0.6154, Loss: 0.6903\n",
      "Epoch   9 Batch   31/78 - Train Accuracy: 0.6234, Validation Accuracy: 0.6185, Loss: 0.6824\n",
      "Epoch   9 Batch   32/78 - Train Accuracy: 0.6051, Validation Accuracy: 0.6137, Loss: 0.6870\n",
      "Epoch   9 Batch   33/78 - Train Accuracy: 0.6281, Validation Accuracy: 0.6166, Loss: 0.6767\n",
      "Epoch   9 Batch   34/78 - Train Accuracy: 0.6174, Validation Accuracy: 0.6166, Loss: 0.6882\n",
      "Epoch   9 Batch   35/78 - Train Accuracy: 0.6154, Validation Accuracy: 0.6151, Loss: 0.7094\n",
      "Epoch   9 Batch   36/78 - Train Accuracy: 0.6115, Validation Accuracy: 0.6169, Loss: 0.6903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 Batch   37/78 - Train Accuracy: 0.6240, Validation Accuracy: 0.6185, Loss: 0.6876\n",
      "Epoch   9 Batch   38/78 - Train Accuracy: 0.6143, Validation Accuracy: 0.6171, Loss: 0.6994\n",
      "Epoch   9 Batch   39/78 - Train Accuracy: 0.6163, Validation Accuracy: 0.6167, Loss: 0.6814\n",
      "Epoch   9 Batch   40/78 - Train Accuracy: 0.5966, Validation Accuracy: 0.6186, Loss: 0.7216\n",
      "Epoch   9 Batch   41/78 - Train Accuracy: 0.6103, Validation Accuracy: 0.6218, Loss: 0.7055\n",
      "Epoch   9 Batch   42/78 - Train Accuracy: 0.6430, Validation Accuracy: 0.6164, Loss: 0.6483\n",
      "Epoch   9 Batch   43/78 - Train Accuracy: 0.6008, Validation Accuracy: 0.6164, Loss: 0.7078\n",
      "Epoch   9 Batch   44/78 - Train Accuracy: 0.6217, Validation Accuracy: 0.6177, Loss: 0.6934\n",
      "Epoch   9 Batch   45/78 - Train Accuracy: 0.5899, Validation Accuracy: 0.6165, Loss: 0.7166\n",
      "Epoch   9 Batch   46/78 - Train Accuracy: 0.6116, Validation Accuracy: 0.6140, Loss: 0.7070\n",
      "Epoch   9 Batch   47/78 - Train Accuracy: 0.6451, Validation Accuracy: 0.6110, Loss: 0.6358\n",
      "Epoch   9 Batch   48/78 - Train Accuracy: 0.6100, Validation Accuracy: 0.6134, Loss: 0.6739\n",
      "Epoch   9 Batch   49/78 - Train Accuracy: 0.5984, Validation Accuracy: 0.6164, Loss: 0.7018\n",
      "Epoch   9 Batch   50/78 - Train Accuracy: 0.5941, Validation Accuracy: 0.6124, Loss: 0.7143\n",
      "Epoch   9 Batch   51/78 - Train Accuracy: 0.6057, Validation Accuracy: 0.6114, Loss: 0.6914\n",
      "Epoch   9 Batch   52/78 - Train Accuracy: 0.6078, Validation Accuracy: 0.6129, Loss: 0.6595\n",
      "Epoch   9 Batch   53/78 - Train Accuracy: 0.5919, Validation Accuracy: 0.6139, Loss: 0.7111\n",
      "Epoch   9 Batch   54/78 - Train Accuracy: 0.6096, Validation Accuracy: 0.6155, Loss: 0.7020\n",
      "Epoch   9 Batch   55/78 - Train Accuracy: 0.6239, Validation Accuracy: 0.6155, Loss: 0.6713\n",
      "Epoch   9 Batch   56/78 - Train Accuracy: 0.6331, Validation Accuracy: 0.6196, Loss: 0.6826\n",
      "Epoch   9 Batch   57/78 - Train Accuracy: 0.6196, Validation Accuracy: 0.6199, Loss: 0.6914\n",
      "Epoch   9 Batch   58/78 - Train Accuracy: 0.6253, Validation Accuracy: 0.6176, Loss: 0.6703\n",
      "Epoch   9 Batch   59/78 - Train Accuracy: 0.6234, Validation Accuracy: 0.6190, Loss: 0.6517\n",
      "Epoch   9 Batch   60/78 - Train Accuracy: 0.6294, Validation Accuracy: 0.6197, Loss: 0.6472\n",
      "Epoch   9 Batch   61/78 - Train Accuracy: 0.6337, Validation Accuracy: 0.6260, Loss: 0.6328\n",
      "Epoch   9 Batch   62/78 - Train Accuracy: 0.6278, Validation Accuracy: 0.6225, Loss: 0.6555\n",
      "Epoch   9 Batch   63/78 - Train Accuracy: 0.6086, Validation Accuracy: 0.6204, Loss: 0.6760\n",
      "Epoch   9 Batch   64/78 - Train Accuracy: 0.6075, Validation Accuracy: 0.6232, Loss: 0.6681\n",
      "Epoch   9 Batch   65/78 - Train Accuracy: 0.6128, Validation Accuracy: 0.6234, Loss: 0.6672\n",
      "Epoch   9 Batch   66/78 - Train Accuracy: 0.6191, Validation Accuracy: 0.6184, Loss: 0.6563\n",
      "Epoch   9 Batch   67/78 - Train Accuracy: 0.6101, Validation Accuracy: 0.6137, Loss: 0.6836\n",
      "Epoch   9 Batch   68/78 - Train Accuracy: 0.5964, Validation Accuracy: 0.6190, Loss: 0.6823\n",
      "Epoch   9 Batch   69/78 - Train Accuracy: 0.5925, Validation Accuracy: 0.6213, Loss: 0.7428\n",
      "Epoch   9 Batch   70/78 - Train Accuracy: 0.6301, Validation Accuracy: 0.6175, Loss: 0.6744\n",
      "Epoch   9 Batch   71/78 - Train Accuracy: 0.6005, Validation Accuracy: 0.6175, Loss: 0.7059\n",
      "Epoch   9 Batch   72/78 - Train Accuracy: 0.6187, Validation Accuracy: 0.6144, Loss: 0.6646\n",
      "Epoch   9 Batch   73/78 - Train Accuracy: 0.6192, Validation Accuracy: 0.6214, Loss: 0.6915\n",
      "Epoch   9 Batch   74/78 - Train Accuracy: 0.6179, Validation Accuracy: 0.6172, Loss: 0.6798\n",
      "Epoch   9 Batch   75/78 - Train Accuracy: 0.6114, Validation Accuracy: 0.6160, Loss: 0.6677\n",
      "Epoch   9 Batch   76/78 - Train Accuracy: 0.6057, Validation Accuracy: 0.6175, Loss: 0.6794\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     source_sequence_length: sources_lengths,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     source_sequence_length: valid_sources_lengths,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Parameters\n",
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checkpoint\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "### Sentence to Sequence\n",
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    word_ids = []\n",
    "    for word in sentence.lower().split():\n",
    "        if word in vocab_to_int:\n",
    "            word_ids.append(vocab_to_int[word])\n",
    "        else:\n",
    "            word_ids.append(vocab_to_int['<UNK>'])\n",
    "    return word_ids\n",
    "\n",
    "\n",
    "test_sentence_to_seq(sentence_to_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/kamalesh_das/Desktop/Python/AcadGild/LanguageTranslator/dev\n",
      "Input\n",
      "  Word Ids:      [208, 154, 121, 11, 60, 37, 190]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [251, 43, 306, 260, 8, 298, 249, 329, 1]\n",
      "  French Words: il a pas le nouveau camion voiture . <EOS>\n"
     ]
    }
   ],
   "source": [
    "### Translate\n",
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "#translate_sentence = 'truck is yellow'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_sequence_length: [len(translate_sentence)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
